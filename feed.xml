<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://3seoksw.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://3seoksw.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-29T00:31:53+00:00</updated><id>https://3seoksw.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Non-Orthogonal Multiple Access (NOMA) - Background</title><link href="https://3seoksw.github.io/blog/2024/NOMA-background/" rel="alternate" type="text/html" title="Non-Orthogonal Multiple Access (NOMA) - Background"/><published>2024-04-08T01:30:00+00:00</published><updated>2024-04-08T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2024/NOMA-background</id><content type="html" xml:base="https://3seoksw.github.io/blog/2024/NOMA-background/"><![CDATA[<h1 id="non-orthogonal-multiple-access-noma---background">Non-Orthogonal Multiple Access (NOMA) - Background</h1> <h2 id="basic-notations">Basic Notations</h2> <ul> <li>\(n\)th user</li> <li>\(k\)th channel</li> <li>\(N_k\): total number of users using \(k\)th channel</li> <li>\(z^k_n\): additive white Gaussian noise (AWGN)</li> <li>\(B_{total}\): total bandwidth</li> <li>\(K\): total number of channels</li> <li>\(N\): total number of users</li> <li>\(B_c=B_{total}/K\): divided bandwidth by channels</li> </ul> <h3 id="channel-to-noise-ratio-cnr">Channel to Noise Ratio (CNR)</h3> <p>CNR is a ratio between the power of the carrier signal (channel) and the power of the noise.</p> \[\begin{align} \Gamma_n^k &amp;= g^o_{n, k} \\ &amp;= \frac{|h^k_n|^2}{\sigma^2_{z_k}} \\ \end{align}\] <ul> <li>\(h^k_n\): channel response between BS and \(n\)th user which considers both the path loss \(\mathcal{P}_L\) and shadowing effect \(h'_{n, k}\) (= Rayleigh fading)</li> <li>\(\sigma^2_{z_k}\): variance of AWGN</li> </ul> <h3 id="signal-to-noise-plus-interference-ratio-sinr">Signal to Noise plus Interference Ratio (SINR)</h3> <blockquote> <p><strong>NOTE</strong>: An assumption behind the following equations is;</p> \[\Gamma^k_1 &gt; ... &gt; \Gamma^k_n &gt; ... &gt; \Gamma^k_{N_k}.\] <p>This means that the 1st user and the last user has the strongest and lowest signal power respectively. According to NOMA protocol, users with lower CNR will be assigned with more power.</p> \[p^k_1 &lt; p^k_2 &lt; ... &lt; p^k_n &lt; ... &lt; p^k_{N_k},\] <p>where \(p^k_n\) denotes transmit power of user \(n\) using channel \(k\).</p> <p>Due to the characteristic of successive interference cancellation (SIC), an user will treat less power as an interference and decode signals of more power.</p> </blockquote> \[\begin{align} \gamma_{n}^{k} &amp;= \frac{p_n^k \Gamma_n^k}{1 + \sum_i^{n-1}{p_i^k \Gamma_n^k}} \\ &amp;= \frac{P_{n, k}(t) \mathcal{P}_L(d)|h'_{n, k(t)}|^2}{n^2_0 + \sum_{i=1}^{n-1}{P_{n, k}(t) \mathcal{P}_L(d)|h'_{n, k(t)}|^2}} \end{align}\] <ul> <li>\(d\): distance between BS and user \(n\) which is using channel \(k\)</li> <li>Typically, \(n^2_0\) and \(1\) represents a constant noise term.</li> <li>Equation (2) is from <code class="language-plaintext highlighter-rouge">TPPD</code> paper.</li> </ul> <p>Overall, SINR is to calculate the ratio between received power (numerator) and other noises and interferences (denominator) which comprises noises (1 or \(n^2_0\)) and interferences (sum of other users’ received power).</p> <p>Take the numerator, by multiplying the allocated power and CNR (\(p^k_n \Gamma^k_n\)), this results a signal power considering the noise.</p> <p>And now let’s take a look at the denominator. The 1 denotes the noise and \(\sum_i^{n-1}p^k_i\Gamma^k_n\) is to sum all the signal powers which are less than \(p^k_n\) (take a look at the <strong>NOTE</strong> assumption).</p> <p>By dividing the above two, ratio of signal power versus noises can be calculated.</p> <h3 id="data-rate">Data Rate</h3> \[R^k_n(\Gamma^k_n, p^k_1, ..., p^k_n) = B_c\log_2{(1+\frac{p^k_n \Gamma^k_n}{1+\sum_{i=1}^{n-1}p^k_i\Gamma^k_n})}.\] <ul> <li>\(\frac{p^k_n \Gamma^k_n}{1+\sum_{i=1}^{n-1}p^k_i\Gamma^k_n}\): As described above, this term represents SINR.</li> <li>\(1 + \frac{p^k_n \Gamma^k_n}{1+\sum_{i=1}^{n-1}p^k_i\Gamma^k_n}\): By adding 1 to SINR, it can prevent logarithm taking zero.</li> <li>\(\log_2{(1+\frac{p^k_n \Gamma^k_n}{1+\sum_{i=1}^{n-1}p^k_i\Gamma^k_n})}\): The term calculates the achievable data rate for a noisy channel and it’s derived from Shannon capacity formula.</li> <li>\(B_c\log_2{(1+\frac{p^k_n \Gamma^k_n}{1+\sum_{i=1}^{n-1}p^k_i\Gamma^k_n})}\): Shannon capacity formula, providing an upper bound on the achievable data rate for a given channel. By multiplying the bandwidth (Hz) and achievable data rate, data rate for the given bandwidth is calculated.</li> </ul> <p>According to the above, two users’ data rate can be derived as such:</p> \[\begin{align} R^k_1(\Gamma^k_1, p^k_1, p^k_2) &amp;= B_c\log_2{(1+p^k_1\Gamma^k_1)} \\ R^k_2(\Gamma^k_2, p^k_1, p^k_2) &amp;= B_c\log_2{(1+ \frac{p^k_2\Gamma^k_2}{1+p^k_1\Gamma^k_2})}. \end{align}\] <h2 id="maximizing-sum-rate-msr">Maximizing Sum Rate (MSR)</h2> <p>Let \(A^k_n=2^{\frac{(R^k_n)_{min}}{B_c}}\) and assume \(A^k_2\geq 2.\)</p> <blockquote> <p>Derivation of \(A^k_n\): Let us assume there is a power \(p'\) which ends up with a minimum data rate \((R^k_n)_{min}\).</p> \[\begin{align*} (R^k_n)_{min} \triangleq B_c\log_2{(1 + \frac{p'\Gamma^k_n}{1+\sum^{n-1}_{i=1}p'\Gamma^k_n})} \end{align*}\] <p>For the sake of simplicity, let’s say the term inside the logarithm as \(P'\), achievable minimum data rate. Then the above equation can be rewritten as</p> \[2^{\frac{(R^k_n)_{min}}{B_c}}=P'=A^k_n.\] <p>MSR is an objective function to ensure the total throughput to be maximized; you can easily consider the MSR metric when the \(\alpha\)-fairness function is in the condition of \(\alpha=1\).</p> </blockquote> <p>For MSR, the power allocation problem is formulated as</p> \[\begin{align} \max_{P} &amp;\sum_{k=1}^{K}[R^k_1(p^k_1, p^k_2) + R^k_2(p^k_1, p^k_2)], \\ \text{subject to } &amp; R^k_n \geq (R^k_n)_{min}, n=1, 2, \forall k = 1, ..., K, \\ &amp; \sum_{k=1}^K(p^k_1 + p^k_2)\leq P_T, \\ &amp; 0 \leq p^k_1\leq p^k_2, \forall k=1, ..., K. \end{align}\] <p>As MSR the term itself describes, the problem is to maximize the sum rate, in other words data rate. MSR’s final objective is to find an optimal power for user 1 and 2.</p> <p>Here, let’s say there is a power limit of \(q_k=p^k_1+p^k_2\), meaning that total allocated power of \(k\)th channel should be \(q_k\). Then the solution for the MSR is</p> \[\begin{align} p^k_1 &amp;= \frac{q_k\Gamma^k_2 - A^k_2 + 1}{A^k_2\Gamma^k_2} \\ p^k_2 &amp;= q_k-p^k_1. \end{align}\] <h2 id="maximizing-minimal-rate-mmr">Maximizing Minimal Rate (MMR)</h2> \[\begin{align} p^k_1 &amp;= \frac{-(\Gamma^k_1 + \Gamma^k_2) + \sqrt{(\Gamma^k_1 + \Gamma^k_2)^2 + 4\Gamma^k_1(\Gamma^k_2)^2q_k}} {2\Gamma^k_1\Gamma^k_2} \\ p^k_2 &amp;= q_k - p^k_1 \end{align}\]]]></content><author><name></name></author><category term="background"/><category term="communications"/><summary type="html"><![CDATA[Basic Theoretical Background of NOMA system]]></summary></entry><entry><title type="html">Pearson Correlation Coefficient (PCC)</title><link href="https://3seoksw.github.io/blog/2024/PCC/" rel="alternate" type="text/html" title="Pearson Correlation Coefficient (PCC)"/><published>2024-02-19T01:30:00+00:00</published><updated>2024-02-19T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2024/PCC</id><content type="html" xml:base="https://3seoksw.github.io/blog/2024/PCC/"><![CDATA[<p><strong>Pearson Correlation Coefficient (PCC)</strong> is a statistic method of measuring linear correlation between two sets of data.</p> <hr/> <h3 id="definition">Definition</h3> <p>Say \(r_{xy}\) is representing a PCC between two variables, \(x\) and \(y\). Then, \(r_{xy}\) is defined as:</p> \[r_{xy} = \frac{\sum_{i=1}^{n}{(x_i - \bar{x})(y_i - \bar{y})}}{\sqrt{\sum_{i=1}^{n}{(x_i - \bar{x})^2}}\sqrt{\sum_{i=1}^{n}{(y_i - \bar{y})^2}}}\] <p>where $n$ is sample size, \(x_i, y_i\) are the samples with the index number of \(i\), \(\bar{x}\) is the sample mean and analogously for \(\bar{y}\). The \(r_{xy}\) is in the range of \([-1, 1]\). As the value gets closer to 1, it represents a strong linear correlation. And conversely when it gets near to -1, it means the two are inversely proportional. Finally, if it is 0, then there is no correlation.</p> <p>The above equation can also be expressed as follows:</p> \[r_{xy} = \frac{cov(x, y)}{\sigma_x \sigma_y}\] <p>where \(cov\) is covariance, and \(\sigma\) is standard deviation.</p>]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><summary type="html"><![CDATA[Statistic method of measuring linear correlation]]></summary></entry><entry><title type="html">Markov Decision Processes</title><link href="https://3seoksw.github.io/blog/2023/MDP/" rel="alternate" type="text/html" title="Markov Decision Processes"/><published>2023-12-27T00:00:00+00:00</published><updated>2023-12-27T00:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/MDP</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/MDP/"><![CDATA[<h1 id="markov-decision-processes">Markov Decision Processes</h1> <h2 id="1-markov-processes">1. Markov Processes</h2> <h3 id="11-introduction-to-mdps">1.1. Introduction to MDPs</h3> <ul> <li><em>Markov Decision Processes</em> formally describe an environment for reinforcement learning.</li> <li>environment is fully observable</li> </ul> <h3 id="12-markov-property">1.2. Markov Property</h3> <ul> <li>the future is independent of the past given the present</li> </ul> <blockquote> <p><strong>Definition</strong><br/> A state \(S_t\) is <em>Markov</em> if and only if \(\mathbb{P}[S_{t+1}|S_t] = \mathbb{P}[S_{t+1}|S_1, ..., S_t]\)</p> </blockquote> <h3 id="13-markov-chains">1.3. Markov Chains</h3> <blockquote> <p><strong>Definition</strong><br/> A <em>Markov Process</em> (or <em>Markov Chain</em>) is tuple \((\mathcal{S}, \mathcal{P})\) \(\mathcal{S}\) is a finite set of states \(\mathcal{P}\) is a state transition probability matrix</p> </blockquote> <h2 id="2-markov-reward-processes">2. Markov Reward Processes</h2> <h3 id="21-markov-reward-process">2.1. Markov Reward Process</h3> <blockquote> <p><strong>Definition</strong><br/> A <em>Markov Reward Process</em> is a tuple \((\mathcal{S}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma})\)</p> </blockquote> <h3 id="22-return">2.2. Return</h3> <blockquote> <p><strong>Definition</strong><br/> \(G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\) where \(\gamma \in [0, 1]\).</p> </blockquote> <h3 id="23-value-function">2.3. Value Function</h3> <blockquote> <p><strong>Definition</strong><br/> \(v(s) = \mathbb{E}[G_t |S_t = s]\) <em>e.g.)</em> \(G_1 = R_2 + \gamma R_3 + ... + \gamma^{T-2}R_T\)</p> </blockquote> <h3 id="24-bellman-equation-for-mrps">2.4. Bellman Equation for MRPs</h3> \[\begin{align*} v(s) &amp;= \mathbb{E}[G_t | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + ...) | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma(G_{t+1}) | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma(v(S_{t+1})) | S_t = s] \\ \end{align*}\] <p align="center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mdp/value-function-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mdp/value-function-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mdp/value-function-1400.webp"/> <img src="/assets/img/mdp/value-function.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> \[v(s) = \mathcal{R}_s + \gamma\sum_{s'\in S}\mathcal{P}_{ss'}v(s')\] <h2 id="3-markov-decision-processes">3. Markov Decision Processes</h2> <h3 id="31-markov-decision-process">3.1. Markov Decision Process</h3> <blockquote> <p><strong>Definition</strong><br/> A <em>Markov Decision Process</em> is a tuple \((\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)\)</p> </blockquote> <h3 id="32-policies">3.2. Policies</h3> <blockquote> <p><strong>Definition</strong><br/> \(\pi(a|s) = \mathbb{P}[A_t = a | S_t = s]\)</p> </blockquote> <h3 id="33-value-function">3.3. Value Function</h3> <blockquote> <p><strong>Definition</strong><br/> <em>state-value function</em><br/> \(v_\pi (s) = \mathbb{E}[G_t | S_t = s]\)</p> </blockquote> <blockquote> <p><strong>Definition</strong><br/> <em>action-value function</em><br/> \(q_\pi(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a]\)</p> </blockquote> <h3 id="34-bellman-expectation-equation">3.4. Bellman Expectation Equation</h3> \[\begin{align*} v_\pi (s) &amp;= \mathbb{E}[G_t | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \end{align*}\] \[\begin{align*} q_\pi(s, a) &amp;= \mathbb{E}[G_t | S_t = s, A_t = a] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \end{align*}\] <p align="center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mdp/state-value-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mdp/state-value-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mdp/state-value-1400.webp"/> <img src="/assets/img/mdp/state-value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> \[\begin{align*} v_\pi(s) = \sum_{a\in \mathcal{A}}{\pi(a|s)q_\pi(s, a)} \end{align*}\] <p align="center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mdp/action-value-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mdp/action-value-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mdp/action-value-1400.webp"/> <img src="/assets/img/mdp/action-value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> \[\begin{align*} q_\pi(s, a) = \mathcal{R}_{s}^{a} + \gamma\sum_{s'\in \mathcal{S}}{\mathcal{P}_{ss'}^{a}v_\pi(s')} \end{align*}\] <p align="center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mdp/combine-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mdp/combine-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mdp/combine-1400.webp"/> <img src="/assets/img/mdp/combine.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> \[\begin{align*} v_\pi(s) = \sum_{a\in \mathcal{A}}{\pi(a|s) \left( \mathcal{R}_{s}^{a} + \gamma\sum_{s'\in \mathcal{S}}{\mathcal{P}_{ss'}^{a}v_\pi(s')} \right)} \end{align*}\]]]></content><author><name></name></author><category term="concepts"/><category term="RL"/><summary type="html"><![CDATA[MDP and MRP]]></summary></entry><entry><title type="html">Evidence Lower Bound (ELBO)</title><link href="https://3seoksw.github.io/blog/2023/ELBO/" rel="alternate" type="text/html" title="Evidence Lower Bound (ELBO)"/><published>2023-11-08T02:00:00+00:00</published><updated>2023-11-08T02:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/ELBO</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/ELBO/"><![CDATA[<h1 id="elbo">ELBO</h1> <p>In <strong>Variational Bayesian Methods</strong>, the Evidence Lower Bound (<strong>ELBO</strong>) is a lower bound on the log-likelihood of some observed data.</p> <hr/> <h3 id="terminology-and-notation">Terminology and Notation</h3> <table> <tbody> <tr> <td>Let \(X\) and \(Z\) be random variables, jointly distributed with distribution \(p_\theta\). For example, \(p_\theta(X)\) is the <strong>Marginal Distribution</strong> of \(X\), and $$p_\theta(Z</td> <td>X)\(is the conditional distribution of\)Z\(given\)X\(. There, for any samle\)x \sim p_\theta\(, and any distribution\)q_\phi$$, we have</td> </tr> </tbody> </table> \[\ln{p_\theta}(x) \geq \mathbb{E}_{z\sim q_\phi}\left[\ln{\frac{p_\theta(x, z)}{q_\phi(z)}}\right].\] <p>LHS: <em>evidence</em> for \(x\) RHS: <em>evidence lower bound (ELBO)</em> for \(x\) The above is refered as the <em>ELBO inequality</em>.</p> <h3 id="applying">Applying</h3> <p>To derive the ELBO, we introduce <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s Inequality</a> applied to randam variables \(x \in X\) here:</p> \[\begin{align} f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)] \end{align}\] <p>We apply <em>Jensen’s Inequality</em> to the \(\log\) (marginal) probability of the observations to get the ELBO.</p> \[\begin{align} \log p(x) &amp;= \log\int_z{p(x, z)dz} \\ &amp;= \log\int_z{p(x, z)\frac{q(z)}{q(z)}dz} \\ &amp;= \log\int_z{\frac{p(x, z)}{q(z)}q(z)dz} \\ &amp;= \log\left({\mathbb{E}_{q(z)}\left[ {\frac{p(x, z)}{q(z)}}\right]}\right) \\ &amp;\geq \mathbb{E}_{q(z)}\left[ \log{\frac{p(x, z)}{q(z)}} \right] \\ &amp;= \mathbb{E}_{q(z)}\left[ \log{p(x, z)} \right] - \mathbb{E}_{q(z)}[\log{q(z)}] \end{align}\] <p>All together, the ELBO for a probability model \(p(x, z)\) and an approximation \(q(z)\) to the posterior is: \(\mathbb{E}_{q(z)}[\log{p(x, z)}]-\mathbb{E}_{q(z)}[\log{q(z)}]\)</p>]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[a lower bound on the log-likelihood]]></summary></entry><entry><title type="html">Kullback-Leibler Divergence (KLD)</title><link href="https://3seoksw.github.io/blog/2023/KLD/" rel="alternate" type="text/html" title="Kullback-Leibler Divergence (KLD)"/><published>2023-11-08T02:00:00+00:00</published><updated>2023-11-08T02:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/KLD</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/KLD/"><![CDATA[<h1 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h1> <p>Kullback-Leibler Divergence is a type of statistical distance: a measure of how one probability distribution \(P\) is different (or similar) from the other probability distribution \(Q\).</p> <h3 id="notation">Notation</h3> \[\begin{align*} D_{KL}(P || Q) \\ KL(P || Q) \end{align*}\] <h3 id="defintion">Defintion</h3> <p>For discrete probability distributions \(P\) and \(Q\) defined on the same sample space \(\mathcal{X}\), the relative entropy from \(Q\) to \(P\) is defined to be \(KL(P || Q) = \sum_{x\in\mathcal{X}}P(x)\log{\frac{P(x)}{Q(x)}}\) For distributions \(P\) and \(Q\) of a continuous random variable, the relative entropy is defined to be \(KL(P || Q) = \int_{-\infty}^{+\infty}p(x)\log{\frac{p(x)}{q(x)}}dx\)</p> <h3 id="applying-kullback-leibler-divergence-to-bayesian-backpropagation">Applying Kullback-Leibler Divergence to Bayesian Backpropagation</h3> \[\begin{align*} \theta^{*} &amp;= \text{argmin}_{\theta}KL[q(w|\theta) \; || \; P(w|\mathcal{D})] \\ &amp;= \text{argmin}_{\theta}\int{q(w|\theta)\log{\frac{q(w|\theta)}{P(w|\mathcal{D})}}}dw \\ &amp;= \text{argmin}_{\theta}\int{q(w|\theta)\log{\frac{q(w|\theta)P(\mathcal{D})}{P(w)P(\mathcal{D}|w)}}}dw \\ &amp;= \text{argmin}_{\theta}\int{q(w|\theta)\log{\frac{q(w|\theta)}{P(w)P(\mathcal{D}|w)}}}dw \\ &amp;= \text{argmin}_{\theta}\left(\int{q(w|\theta)\log{\frac{q(w|\theta)}{P(w)}}}dw \; - \; \int{q(w|\mathcal{D})\log{P(\mathcal{D}|w)}dw} \right) \\ &amp;= KL[q(w|\theta) \; || \; P(w)] \; - \; \mathbb{E}_{q(w|\mathcal{D})}[\log{P(\mathcal{D}|w)}] \end{align*}\] <p>Complexity cost (prior-dependent part): \(KL[q(w \mid \theta) \mid \mid P(w)]\)</p> <p>Likelihood cost (data-dependent part): \(\mathbb{E}_{q(w \mid \mathcal{D})}[P(\mathcal{D}\mid w)]\)</p> <p>Resulting cost function:</p> \[\mathcal{F}(\mathcal{D}, \theta) = KL[q(w|\theta) || P(w)] - \mathbb{E}_{q(w|\mathcal{D})}[\log P(\mathcal{D}|w)]\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[statistical distance between two distributions]]></summary></entry><entry><title type="html">Cross-Entropy</title><link href="https://3seoksw.github.io/blog/2023/Cross-Entropy/" rel="alternate" type="text/html" title="Cross-Entropy"/><published>2023-11-01T01:30:00+00:00</published><updated>2023-11-01T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/Cross-Entropy</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/Cross-Entropy/"><![CDATA[<p>The <strong>Cross-Entropy</strong> between two probability distributions \(p\) and \(q\) measures the difference for a given random variable or set of events.</p> <h3 id="definition">Definition</h3> <p>The <strong>Cross-entropy</strong> of the distribution \(q\) relative to a distribution \(p\) over a given set is defined as follows:</p> \[H(p, q) = -\mathbb{E}_p[\log q]\] <p>where \(\mathbb{E}_p[\cdot]\) is the expected value operator with respect to the distribution \(p\). The definition may be formulated using the <a href="https://3seoksw.github.io/blog/2023/KLD/">Kullback-Leibler Divergence</a> \(KL[p \mid\mid q]\).</p> \[H(p, q) = H(p) + KL[p \mid\mid q]\] <p>For discrete probability distributions:</p> \[H(P, Q) = -\sum_{x\in\mathcal{X}}p(x*i)\log{q(x_i)}\] <p>For continuous probability distributions:</p> \[H(p, q) = -\int_{\mathcal{X}}P(x)\log{Q(x)}dr(x)\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[A method to measure the difference between two different probability distributions]]></summary></entry><entry><title type="html">Maximum Likelihood Estimation (MLE)</title><link href="https://3seoksw.github.io/blog/2023/MLE/" rel="alternate" type="text/html" title="Maximum Likelihood Estimation (MLE)"/><published>2023-09-15T01:30:00+00:00</published><updated>2023-09-15T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/MLE</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/MLE/"><![CDATA[<h1 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h1> <p>A maximum likelihood estimation (MLE) is a method of estimating the parameters of the given likelihood probability distribution.</p> <h3 id="definition">Definition</h3> <p>A value of $\theta$ that maximizes \(L(\theta|x_1, x_2, ..., x_n)\). Most likely, natural log will be plugged (<a href="https://3seoksw.github.io/blog/2023/likelihood">Log-Likelihood Function</a>).</p> \[\begin{align*} \theta^* &amp;= \text{argmax}_\theta l(\theta|x_1, x_2, ..., x_n) \\ &amp;= \text{argmax}_\theta log(\mathcal{L}(\theta|x_1, x_2, ..., x_n)) \\ &amp;= \text{argmax}_\theta log(\prod_{i=1}^{n}f(x_i|\theta)) \\ &amp;= \text{argmax}_\theta log(f(x_1|\theta) \times f(x_2|\theta) \times ... \times f(x_n|\theta)) \\ &amp;= \text{argmax}_\theta \sum_{i=1}^{n}log(f(x_i|\theta)) \end{align*}\] <p>From <strong>Bayesian Backpropagation</strong>:</p> \[\begin{align*} w^\text{MLE} &amp;= \text{argmax}_wl(w|\mathcal{D}) \\ &amp;= \text{argmax}_w\log{\mathcal{L}(w|\mathcal{D})} \\ &amp;= \text{argmax}_w\log{P(\mathcal{D}|w)} \\ &amp;= \text{argmax}_w\log{P(\mathcal{D_1}, ...\mathcal{D_n}|w)} \\ &amp;= \text{argmax}_w\log{\prod_{i=1}^{n}P(\mathcal{D_i}|w)} \\ &amp;= \text{argmax}_w\sum_{i=1}^{n}{\log{P(\mathcal{D_i}|w)}} \\ &amp;= \text{argmax}_w\sum_{i=1}^{n}{\log{P(y_i|x_i,w)}} \\ \end{align*}\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[estimation method of a likelihood distribution]]></summary></entry><entry><title type="html">Bayes’ Theorem</title><link href="https://3seoksw.github.io/blog/2023/bayes-theorem/" rel="alternate" type="text/html" title="Bayes’ Theorem"/><published>2023-09-14T01:30:00+00:00</published><updated>2023-09-14T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/bayes-theorem</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/bayes-theorem/"><![CDATA[<h1 id="bayes-theorem">Bayes’ Theorem</h1> <p><strong>Bayes’ Theorem</strong> (or Bayesian Theorem) is a statistical method to update our prior beliefs. Bayes theorem can be used in various fields such as in Machine Learning (ML) method.</p> <hr/> <h3 id="definition">Definition</h3> \[\begin{align*} \text{Posterior} \propto \text{Prior} \times \text{Likelihood} \\ P(A|B) = \frac{P(B|A) \times P(A)}{P(B)} \\ \end{align*}\] <h3 id="usage-in-ml">Usage in ML</h3> \[\begin{align*} P(w|\mathcal{D}) \propto P(\mathcal{D}|w) \times P(w) \\ \end{align*}\] <p>\(w\): Prior weights for a neural network<br/> \(\mathcal{D}\): Data<br/> \(P(w|\mathcal{D})\): Posterior distribution, a probability distribution of the neural network weights \(w\) after observing the data \(\mathcal{D}\)<br/> \(P(\mathcal{D}|w)\): Likelihood function, represents how well the neural network with parameters \(w\) fits the observed data \(\mathcal{D}\)<br/> \(P(w)\): Prior, prior beliefs about the neural network weights before observing the data \(\mathcal{D}\)<br/> \(P(y^*|x^*)=\mathbb{E}_{P(w|\mathcal{D})}[P(y^*|x^*, w)]\) At prediction time, the predictive distribution over the target \(y^*\) given a test input \(x^*\)<br/></p>]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[Bayes Rule, probability of an event based on prior knowledge]]></summary></entry><entry><title type="html">Likelihood Function</title><link href="https://3seoksw.github.io/blog/2023/likelihood/" rel="alternate" type="text/html" title="Likelihood Function"/><published>2023-09-14T01:30:00+00:00</published><updated>2023-09-14T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/likelihood</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/likelihood/"><![CDATA[<h1 id="likelihood-function">Likelihood Function</h1> <p>The likelihood function is the joint probability of the given data (say \(x\)) viewed as a function.</p> <h3 id="definition">Definition</h3> \[\begin{align*} &amp; L(\theta|x_1, x_2, ..., x_n) \\ &amp;= \text{joint pmf/pdf of random variables } x_1, x_2, ..., x_n \text{ from } \theta \\ &amp;= f(x_1, x_2, ..., x_n|\theta) \\ &amp;= f(x_1|\theta) \times f(x_2|\theta) \times ... \times f(x_n|\theta) \\ &amp;= \prod_{i=1}^{n}f(x_i|\theta) \\ \end{align*}\] <h3 id="log-likelihood-function">Log-Likelihood Function</h3> <p>Plugging the <strong>Likelihood function</strong> into a logarithm shows as follows:</p> \[\begin{align*} &amp;l(\theta|x_1, x_2, ..., x_n) \\ &amp;= log(L(\theta|x_1, x_2, ..., x_n)) \\ &amp;= log(\prod_{i=1}^{n}f(x_i|\theta)) \\ &amp;= \sum_{i=1}^{n}{log{f(x_i|\theta)}} \end{align*}\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[likelihood function]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>