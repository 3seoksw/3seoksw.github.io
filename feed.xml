<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://3seoksw.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://3seoksw.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-15T23:57:45+00:00</updated><id>https://3seoksw.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">COLLABLLM, From Passive Responders to Active Collaborators</title><link href="https://3seoksw.github.io/blog/2025/COLLABLLM-review/" rel="alternate" type="text/html" title="COLLABLLM, From Passive Responders to Active Collaborators"/><published>2025-11-12T00:00:00+00:00</published><updated>2025-11-12T00:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2025/COLLABLLM-review</id><content type="html" xml:base="https://3seoksw.github.io/blog/2025/COLLABLLM-review/"><![CDATA[<p>The review is done with the following paper and the figures used for this article are derived from the paper:<br/> <a href="https://arxiv.org/abs/2502.00640">COLLABLLM: From Passive Responders to Active Collaborators</a> <d-cite key="collabllm"></d-cite>.</p> <h2 id="abstract">Abstract</h2> <p>COLLABLLM‚Äôs motivation and its idea are straightforward and intuitive.</p> <p>I believe you might also have experienced this. Imagin you‚Äôre using a large language model like Chat-GPT, and you ask some question to the model then you‚Äôve probably gotten a passive answer at first. Then you had to refine your instruction since the model‚Äôs initial answer wasn‚Äôt satisfactory and repeat this process multiple times, which is quite frustrating experience and consumes a lot of time. This largely comes from how most models are trained and optimized for single-turn helpfulness, which encourages passive responses and ultimately limits long-term interaction. COLLABLLM aims for a more interactive, multiturn-based experience. To address the aforementioned problems, COLLABLLM introduces two ideas which are Collaborative simulation and a Multiturn-aware Reward which is called MR. Together, these make the model more proactive and lead to higher task performance and better interactivity across multiple turns.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/comp_collabllm-480.webp 480w,/assets/img/COLLABLLM/comp_collabllm-800.webp 800w,/assets/img/COLLABLLM/comp_collabllm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/comp_collabllm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1. Comparisons between existing LLMs and COLLABLLM. <d-cite key="collabllm"></d-cite> </div> <h2 id="overview-of-collabllm">Overview of COLLABLLM</h2> <p>The following figure shows the overall process of how COLLABLLM works.</p> <p>For instance, the user gives an instruction to the model asking to write about how optimism can improve our well-being. Given the context state x, the model outputs a response y from the policy pi. What differs from other models is that instead of giving the answer right away, it asks more contextual questions like what kind of tone are you aiming for.</p> <p>The way model is not giving you passive answer, rather asking contextual questions is possible because the model is using multiturn-aware reward function to ensure model is accounting for multiturn. Also, making the model be aware of future turns is done by using collaborative simulation to see what would the future conversation be like.</p> <p>Collaborative simulation is to sample future conversations given the context state. You can think of collaborative simulation as a conversation lookahead method between user and the model. However, it is impossible to know what would users ask in near future, and even corresponding replies to unknown user‚Äôs future input.</p> <p>To make this feasible for enabling lookahead of conversations, a simulated user such as GPT 4o that imitates the actual user generates user‚Äôs future input. In collaborative simulation, forward sampling is used to retrieve unknown future dialogue between the user and the model. Ultimately, this enables the model take future conversation into account and choose responses aligned with a long-term goal, instead of a current-focused goal.</p> <p>Finally, they apply reinforcement learning fine-tuning such as SFT, PPO, and DPO using the MR function.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/framework-480.webp 480w,/assets/img/COLLABLLM/framework-800.webp 800w,/assets/img/COLLABLLM/framework-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/framework.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2. COLLABLLM Framework. </div> <h2 id="problem-formulation">Problem Formulation</h2> <ul> <li>Multiple Turns</li> </ul> \[\begin{align*} &amp; t_j = \{u_j, m_j \}, \text{ at turn } j=1, \cdots, K \\ &amp; u_j: \text{ user input} \\ &amp; m_j: \text{ model output} \\ &amp; t_{1:j} = \{ t_1, \cdots, t_j \} \\ &amp; t_{j}^{h} = t_{1:j} \cup \{u_j\} \end{align*}\] <ul> <li>Multiturn-aware Reward (MR)</li> </ul> \[\begin{align*} &amp; R^{*}(t_{1:K}|g): \text{ objective given some ground-truth goal} \\ &amp; MR(m_j | t_j^{h}, g) = \mathbb{E}_{t_j^f\sim P(t_j^f | t_{1:j})}[R^{*}(t_{1:j}\cup t_j^h | g)] \\ &amp; t_j^f = t_{j+1:K}: \text{ forward trajectory following } j\text{-th turn} \end{align*}\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/MR_simulation-480.webp 480w,/assets/img/COLLABLLM/MR_simulation-800.webp 800w,/assets/img/COLLABLLM/MR_simulation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/MR_simulation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3. Multiturn-aware Rewards from collaborative simulation. </div> <h3 id="conversation-level-reward">Conversation-level Reward</h3> \[R^{*}(t|g) = R_{\text{ext}}(t, g) + R_{\text{int}}(t)\] <ul> <li>Extrinsic Reward <ul> <li>\(R_{\text{ext}}(t, g) = S(\text{Extract}(t), y_g)\) evaluates how well the conversation achieves the user‚Äôs goal \(g\)</li> <li>\(S(\cdot, \cdot)\) evaluates task-specific metrics (<em>e.g.</em>, accuracy or similarity)</li> <li>\(\text{Extract(t)}\) extracts the final response (solution) from the conversation \(t\)</li> </ul> </li> <li>Intrinsic Reward <ul> <li>\(R_{\text{int}}(t) = - min[\lambda \cdot \text{TokenCount}(t), 1] + R_{LLM}(t)\), comprises penalty and helpfulness</li> <li>\(\lambda\) controls the penalty for the number of tokens being used</li> <li>\(R_{LLM}\) evaluates user-valued objectives (<em>e.g.</em>, engagement or interactivity)</li> </ul> </li> </ul> <p>The above uses LLM as a judge in terms of evaluating the intrinsic variables <d-cite key="zheng2023judging"><d-cite>.</d-cite></d-cite></p> <h3 id="forward-sampling">Forward Sampling</h3> \[t_j^f \sim P(t_j^f | t_{1:j})\] <ul> <li>User Simulator <ul> <li>Simulator \(U\) generates a probabilistic distribution \(P(u \mid t)\)</li> </ul> </li> <li>Sampling Method <ul> <li>Naive approach is to use Monte Carlo Sampling \(\rightarrow\) computationally expensive</li> <li>Introduce a window size \(w\) (trade objectives for huge cost savings)</li> </ul> </li> </ul> \[t_j^{f_w} = t_{j+1:j+w} \leftrightarrow t_j^f = t_{j+1:K}\] <h2 id="experiments">Experiments</h2> <h3 id="experimental-setup">Experimental Setup</h3> <p>COLLABLLM is based on Llama 3.1, and it has four variants. First two are offline models, supervised fine-tuning model and offline DPO model. Offline models only use the pre-determined datasets to update the model‚Äôs policy network during training. Then from the first two models, these two are further trained to online models, PPO and online DPO model. Online models are participating in the simulation to compute new MRs and update the policy network during training.</p> <p>So the difference between offline models and online models is that offline models do not participate in the simulation and online models do.</p> <p>Two baseline models will be used in the paper. One is called based model which is vanila Llama-3.1. The second one is called proactive base model, and it is a base model with proactive prompt engineering model. Proactive base model is simply base model given with the prompt as such figure 4 to be more collaborative and interactive.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/MR-480.webp 480w,/assets/img/COLLABLLM/MR-800.webp 800w,/assets/img/COLLABLLM/MR-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/MR.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 4. Generating high-quality conversation data with Multiturn-aware Rewards (MR). </div> <p>And COLLABLLM is evaluated with the baseline models in three different environment datasets.</p> <p>First is MediumDocEdit-Chat dataset focusing on document editing sampled from Medium articles. It is evaluated with BLEU score measuring similarity between the extracted document and the original article.</p> <p>Second is BigCodeBench-Chat dataset meant for coding assistance. It is sampled from BigCodeBench dataset and is using pass rate as an evaluation metric.</p> <p>Finally, MATH-chat dataset is used and it‚Äôs sampled from MATH dataset. The task is evaluated with the accuracy metric.</p> <p>In addition to the task-specific metrics, two task-agnostic metrics are incorporated, one is average token count and the other is interactivity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/simulated_env-480.webp 480w,/assets/img/COLLABLLM/simulated_env-800.webp 800w,/assets/img/COLLABLLM/simulated_env-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/simulated_env.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 5. Simulated Multiturn Environment Datasets. </div> <h3 id="results-of-simulated-experiments">Results of Simulated Experiments</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/tab1-480.webp 480w,/assets/img/COLLABLLM/tab1-800.webp 800w,/assets/img/COLLABLLM/tab1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/tab1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Table 1. Evaluation results on our multiturn datasets. Green zone: Baselines; Orange zone: Variants of COLLABLLMs. Rel. Improv. indicates the relative improvements of COLLABLLMs trained with Online DPO over Proactive Base. </div> <h3 id="ablation-study-on-reward-mechanism">Ablation Study on Reward Mechanism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/ablation-480.webp 480w,/assets/img/COLLABLLM/ablation-800.webp 800w,/assets/img/COLLABLLM/ablation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/ablation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 6. Ablation Study of Reward Mechanisms on MediumDocEdit-Chat. This figure compares three immediate reward mechanisms with three MR variants. </div> <h3 id="real-world-user-study">Real-world User Study</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/real-world-480.webp 480w,/assets/img/COLLABLLM/real-world-800.webp 800w,/assets/img/COLLABLLM/real-world-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/real-world.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 7. Real-world user study includes 201 participants interacting with Base, Proactive Base, and COLLABLLM. (a) document quality (b) overall interaction experience (c) spent time (d) additional assessments every three turns. </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Most LLMs make passive and short-sighted output due to single-turn training</li> <li>Add a future lookahead</li> <li>COLLABLLM introduces collaborative simulator and multiturn-aware reward (MR) \(\rightarrow\) Shows effectiveness, efficiency, engagement throughout extensive simulated and real-world evaluations.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="LLM"/><summary type="html"><![CDATA[Multiturn-aware LLM aiming for long-term goal]]></summary></entry><entry><title type="html">LION, Empowering MLLM with Dual-Level Visual Knowledge</title><link href="https://3seoksw.github.io/blog/2025/LION-review/" rel="alternate" type="text/html" title="LION, Empowering MLLM with Dual-Level Visual Knowledge"/><published>2025-10-09T00:00:00+00:00</published><updated>2025-10-09T00:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2025/LION-review</id><content type="html" xml:base="https://3seoksw.github.io/blog/2025/LION-review/"><![CDATA[<p>The review is done with the following paper:<br/> <a href="https://openaccess.thecvf.com/content/CVPR2024/liquid/Chen_LION_Empowering_Multimodal_Large_Language_Model_with_Dual-Level_Visual_Knowledge_CVPR_2024_paper.html">LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge</a>.</p> <h2 id="abstract">Abstract</h2> <p>Existing MLLMs mainly use vision encoders that are pre-trained on coarsely aligned image-text pairs, which often leads to vague and inaccurate responses. These issues are due to the insufficient extraction and reasoning of visual knowledge. In other words, the existing models struggle from region-level tasks.</p> <p>To tackle this problem, the paper proposes the LION model. The objective of the model is to inject two levels of visual knowledge, which are image-level and region-level understanding. To make this possible, the model incorporates fine-grained spatial-aware knowledge, and applies soft prompting of high-level semantic visual evidence. These two enable the MLLM¬† to capture both global and local visual information from a given image.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/LION_MLLM_comp-480.webp 480w,/assets/img/LION/LION_MLLM_comp-800.webp 800w,/assets/img/LION/LION_MLLM_comp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/LION_MLLM_comp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 1.</i> Comparison between existing MLLMs and LION <a href="#1">[1]</a>. </p> <h2 id="related-works--visual-grounding">Related Works ‚Äì Visual Grounding</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/bbox_rep-480.webp 480w,/assets/img/LION/bbox_rep-800.webp 800w,/assets/img/LION/bbox_rep-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/bbox_rep.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 2.</i> Representation of object description and bounding box which follows Markdown link format <a href="#2">[1]</a>. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/kosmos-2-480.webp 480w,/assets/img/LION/kosmos-2-800.webp 800w,/assets/img/LION/kosmos-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/kosmos-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure 3. Kosmos-2 offering object description with bounding box <d-cite key="peng2023kosmos"></d-cite>.</p> <p>While there are numerous works on assigning visual grounding tasks to MLLMs, Kosmos-2 is a great example for comparison with the LION model. Kosmos-2 converts existing datasets into a Markdown-style link format. This format represents a bounding box which includes spatial coordinates enclosed in square brackets as shown in Figure 2 and 3.</p> <p>The reason this Markdown link format matters is that it provides a tokenizer-friendly representation, making it easier for the model to understand both the semantic meaning of the image tags and their spatial locations. In this way, it effectively bridges the gap between text and visual grounding tasks. However, the Kosmos-2 still fall short in handling broader aspect of visual tasks beyond region-level grounding.</p> <h2 id="method">Method</h2> <h3 id="pipeline">Pipeline</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/lion_overview-480.webp 480w,/assets/img/LION/lion_overview-800.webp 800w,/assets/img/LION/lion_overview-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/lion_overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 4.</i> Overview of LION <a href="#1">[1]</a>. </p> <h3 id="spatial-visual-knowledge">Spatial Visual Knowledge</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/lion_spatial-480.webp 480w,/assets/img/LION/lion_spatial-800.webp 800w,/assets/img/LION/lion_spatial-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/lion_spatial.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 5.</i> Representation of how LION handle spatial visual knowledge <a href="#1">[1]</a>. </p> <p>To incorporate spatial-aware visual knowledge into the model, the paper suggests reformatting the datasets into a unified format, that combines natural language descriptions and object coordinates enclosed in square brackets for instruction tuning, just like Kosmos-2 did.</p> <p>Still, it remains two main internal conflicts when the model tries to learn both image-level and region-level visual tasks. One is the need of region-level modality-alignment pre-training. This is because most MLLMs are only trained with global image features. And the second is the gap between the input-output modes of image-level and region level visual tasks. As mentioned earlier, the reformatted data contains text and coordinates, which can confuse the model when trained together.</p> <p>To address these two conflicts, a stage-wise instruction tuning strategy is applied, which is a three-stage training strategy.</p> <ol> <li> <p>Image-level For the first stage, the model learns general vision-language understanding, by fine-tuning the Q-Former and the image-level adapter in the LLM. This offers the model with image-level knowledge.</p> </li> <li> <p>Region-level And for the second stage, the model focuses on fine-grained spatial knowledge, by using a vision aggregator to capture detailed visual features along with the region-level adapter. At this stage, the model learns more about region-level knowledge, and as the model is trained on region-level AFTER the image-level, it can avoid severe interference between the two levels.</p> </li> <li> <p>Mixture of the both Finally at the third stage, the model combines the outputs from the previous two stages using a router, which dynamically balances image-level and region-level knowledge. The key component here is the router. The router not only balances between the two levels of knowledge,¬†but also aligns the input-output mismatch by assigning adaptive weights to each adapters‚Äô output based on the task type.</p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/stage-wise_instruction-tuning-480.webp 480w,/assets/img/LION/stage-wise_instruction-tuning-800.webp 800w,/assets/img/LION/stage-wise_instruction-tuning-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/stage-wise_instruction-tuning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 6.</i> The stage-wise instruction-tuning strategy <a href="#1">[1]</a>. </p> \[\begin{align*} O^t = F(X) + \sum_{k=1}^{K=2}G_k^t \odot H_k(X), \end{align*}\] <p>where \(H_k(X)\) is an adapter for \(k\)-th adapter and \(F(X)\) is the output of FFN.</p> <p>To make the stage-wise instruction possible, we must first look at the placement of the adapters in the LLM. Each adapter is inserted at every Feed-Forward Network layer in a parallel manner within the frozen LLM. In this arrangement, the output features generated by the standard FFN are simply added to the output features generated by the adapter layer.</p> <p>Sine each adapter is trained separately, we treat these specialized components as distinct experts.</p> <p>However, as previously mentioned, the router is the key component that enables the model to use these adapters effectively. The router module dynamically decides how much to rely on each adapter based on the task by learning a weight vector G for every task.</p> <p>¬† For example, if the input involves spatial reasoning, the router increases the contribution of the region-level adapter by updating its weight, and vice versa for global reasoning.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/vision_aggregator-480.webp 480w,/assets/img/LION/vision_aggregator-800.webp 800w,/assets/img/LION/vision_aggregator-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/vision_aggregator.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 7.</i> Vision Aggregator. </p> <p>The next challenge is to ensure that the model can capture sufficient visual details. To capture the fine-grained spatial-aware visual knowledge needed for tasks like visual grounding, the paper introduced a component called the Vision Aggregator.</p> <p>The Vision Aggregator functions as a tiny transformer-style network where that improves LION‚Äôs understanding of object boundaries, spatial relations, and fine object attributes.</p> <p>Ablation studies demonstrate that the VA promotes the extraction of this fine-grained knowledge and significantly improves referring expression comprehension (REC) performance.</p> <h3 id="soft-prompting">Soft Prompting</h3> <h4 id="image-tag-extraction">Image Tag Extraction</h4> <p>To improve LION‚Äôs capabilities, the paper included semantic comprehension. The authors used an off-the-shelf image tag extractor called Recognize Anything Model (RAM).</p> <h4 id="soft-prompting-1">Soft prompting</h4> <p>Since the predicted tags from this model are not flawless, they can mislead the model. So, LION uses a soft prompt to mitigate the influence of these imperfect tags. A trainable embedding is added to the instruction text that teaches the model how to use the tag information. In the paper, the phrase ‚ÄúAccording to <i>hint</i>, you are allowed to use or partially use the following tags: ‚Ä¶‚Äù. This method helps guide the model to select valuable information from the tags and ignore the incorrect ones.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/soft-prompting-480.webp 480w,/assets/img/LION/soft-prompting-800.webp 800w,/assets/img/LION/soft-prompting-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/soft-prompting.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 8.</i> Instruction template with soft prompt <a href="#1">[1]</a>. </p> <h2 id="experimental-results">Experimental Results</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/tab1-480.webp 480w,/assets/img/LION/tab1-800.webp 800w,/assets/img/LION/tab1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/tab1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Table 1.</i> Comparison on image vaptioning and Visual Question Answering (VQA). The best and second performances for each benchmark are indicated in bold and underline, respectively <a href="#1">[1]</a>. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/tab2-480.webp 480w,/assets/img/LION/tab2-800.webp 800w,/assets/img/LION/tab2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/tab2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Table 2.</i> Comparison on Referring Expression Comprehension (REC) <a href="#1">[1]</a>. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/tab3-480.webp 480w,/assets/img/LION/tab3-800.webp 800w,/assets/img/LION/tab3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/tab3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Table 3.</i> Evaluation of object hallucination <a href="#1">[1]</a>. </p> <p>The LION architecture effectively addresses the challenge of insufficient visual knowledge extraction and reasoning, which affects existing Multimodal Large Language Models (MLLMs) that typically rely only on coarsely aligned image-text pairs.</p> <p>¬† The core innovation of LION is the injection of dual-level visual knowledge where, first, the Fine-grained spatial-aware knowledge is incorporated using the mixture-of-adapters using a router and a Vision Aggregator, and, second, the High-level semantic visual evidence is provided by image tags through a soft prompting method.</p> <h2 id="reference">Reference</h2> <p><a id="1" href="https://openaccess.thecvf.com/content/CVPR2024/liquid/Chen_LION_Empowering_Multimodal_Large_Language_Model_with_Dual-Level_Visual_Knowledge_CVPR_2024_paper.html">[1]</a> Chen, G., Shen, L., Shao, R., Deng, X., &amp; Nie, L. (2024). Lion: Empowering multimodal large language model with dual-level visual knowledge. In <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> (pp. 26540-26550).</p> <p><a id="2" href="https://arxiv.org/abs/2306.14824">[1]</a> Peng, Z., Wang, W., Dong, L., Hao, Y ., Huang, S., Ma, S., &amp; Wei, F. (2023). Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824.</p>]]></content><author><name></name></author><category term="paper-review"/><category term="MLLM"/><summary type="html"><![CDATA[Paper review of LION]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://3seoksw.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://3seoksw.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We‚Äôre introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we‚Äôre introducing Gemini 1.5 Flash: a model that‚Äôs lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We‚Äôre also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5‚Äôs 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It‚Äôs optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it‚Äôs a lighter weight model than 1.5 Pro, it‚Äôs highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it‚Äôs been trained by 1.5 Pro through a process called ‚Äúdistillation,‚Äù where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash‚Äôs availability and pricing.Over the last few months, we‚Äôve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we‚Äôve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We‚Äôve improved control over the model‚Äôs responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we‚Äôve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we‚Äôre now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do ‚Äî not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we‚Äôre also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We‚Äôre announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we‚Äôve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind‚Äôs mission to build AI responsibly to benefit humanity, we‚Äôve always wanted to develop universal AI agents that can be helpful in everyday life. That‚Äôs why today, we‚Äôre sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do ‚Äî and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we‚Äôve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we‚Äôve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we‚Äôve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they‚Äôre being used in, and respond quickly, in conversation.With technology like this, it‚Äôs easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We‚Äôve made incredible progress so far with our family of Gemini models, and we‚Äôre always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we‚Äôre able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google‚Äôs privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let‚Äôs stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><category term="external-posts"/><category term="google"/><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio¬†Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website!¬†üéâüéâ</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as¬†sources.</p> <p>Any questions or suggestions? üëâ Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on¬†GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry></feed>