---
layout: post
title: Evidence Lower Bound (ELBO)
date: 2023-11-07 22:00:00-0400
description: a lower bound on the log-likelihood
tags: statistics bayes
categories: concepts
giscus_comments: false
related_posts: false
related_publications:
---

# ELBO

In **Variational Bayesian Methods**, the Evidence Lower Bound (**ELBO**) is a lower bound on the log-likelihood of some observed data.

---

### Terminology and Notation

Let $$X$$ and $$Z$$ be random variables, jointly distributed with distribution $$p_\theta$$. For example, $$p_\theta(X)$$ is the **Marginal Distribution** of $$X$$, and $$p_\theta(Z \; | \; X)$$ is the conditional distribution of $$Z$$ given $$X$$. There, for any samle $$x \; \sim \; p_\theta$$, and any distribution $$q_\phi$$, we have

$$
\ln{p_\theta}(x) \geq \mathbb{E}_{z\sim q_\phi}\left[\ln{\frac{p_\theta(x, z)}{q_\phi(z)}}\right].
$$

LHS: _evidence_ for $$x$$
RHS: _evidence lower bound (ELBO)_ for $$x$$
The above is refered as the _ELBO inequality_.

### Applying

To derive the ELBO, we introduce [Jensen’s Inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) applied to randam variables $$x \in X$$ here:

$$
\begin{align}
	f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]
\end{align}
$$

We apply _Jensen’s Inequality_ to the $$\log$$ (marginal) probability of the observations to get the ELBO.

$$
\begin{align}
	\log p(x)
	&= \log\int_z{p(x, z)dz} \\
	&= \log\int_z{p(x, z)\frac{q(z)}{q(z)}dz} \\
	&= \log\int_z{\frac{p(x, z)}{q(z)}q(z)dz} \\
	&= \log\left({\mathbb{E}_{q(z)}\left[ {\frac{p(x, z)}{q(z)}}\right]}\right) \\
	&\geq \mathbb{E}_{q(z)}\left[
	\log{\frac{p(x, z)}{q(z)}}
	\right] \\
	&= \mathbb{E}_{q(z)}\left[
	\log{p(x, z)}
	\right]
	- \mathbb{E}_{q(z)}[\log{q(z)}]
\end{align}
$$

All together, the ELBO for a probability model $p(x, z)$ and an approximation $q(z)$ to the posterior is:
$$\mathbb{E}_{q(z)}[\log{p(x, z)}]-\mathbb{E}_{q(z)}[\log{q(z)}]$$
