<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LION, Empowering MLLM with Dual-Level Visual Knowledge | WooSeok Kim </title> <meta name="author" content="WooSeok Kim"> <meta name="description" content="Dual-level visual knowledge equipped MLLM with soft prompting"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://3seoksw.github.io/blog/2025/LION-review/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "LION, Empowering MLLM with Dual-Level Visual Knowledge",
            "description": "Dual-level visual knowledge equipped MLLM with soft prompting",
            "published": "October 09, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">WooSeok</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Paper Reviews </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>LION, Empowering MLLM with Dual-Level Visual Knowledge</h1> <p>Dual-level visual knowledge equipped MLLM with soft prompting</p> </d-title> <d-article> <p>The review is done with the following paper:<br> <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Chen_LION_Empowering_Multimodal_Large_Language_Model_with_Dual-Level_Visual_Knowledge_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge</a> <d-cite key="lion"></d-cite>.</p> <h2 id="abstract">Abstract</h2> <p>Existing MLLMs mainly use vision encoders that are pre-trained on coarsely aligned image-text pairs, which often leads to vague and inaccurate responses. These issues are due to the insufficient extraction and reasoning of visual knowledge. In other words, the existing models struggle from region-level tasks.</p> <p>To tackle this problem, the paper proposes the LION model. The objective of the model is to inject two levels of visual knowledge, which are image-level and region-level understanding. To make this possible, the model incorporates fine-grained spatial-aware knowledge, and applies soft prompting of high-level semantic visual evidence. These two enable the MLLM  to capture both global and local visual information from a given image.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/LION_MLLM_comp-480.webp 480w,/assets/img/LION/LION_MLLM_comp-800.webp 800w,/assets/img/LION/LION_MLLM_comp-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/LION_MLLM_comp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Figure 1.</i> Comparison between existing MLLMs and LION <a href="#1">[1]</a>. </p> <h2 id="related-works--visual-grounding">Related Works – Visual Grounding</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/bbox_rep-480.webp 480w,/assets/img/LION/bbox_rep-800.webp 800w,/assets/img/LION/bbox_rep-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/bbox_rep.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Figure 2.</i> Representation of object description and bounding box which follows Markdown link format <a href="#2">[1]</a>. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/kosmos-2-480.webp 480w,/assets/img/LION/kosmos-2-800.webp 800w,/assets/img/LION/kosmos-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/kosmos-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure 3. Kosmos-2 offering object description with bounding box <d-cite key="peng2023kosmos"></d-cite>.</p> <p>While there are numerous works on assigning visual grounding tasks to MLLMs, Kosmos-2 is a great example for comparison with the LION model. Kosmos-2 converts existing datasets into a Markdown-style link format. This format represents a bounding box which includes spatial coordinates enclosed in square brackets as shown in Figure 2 and 3.</p> <p>The reason this Markdown link format matters is that it provides a tokenizer-friendly representation, making it easier for the model to understand both the semantic meaning of the image tags and their spatial locations. In this way, it effectively bridges the gap between text and visual grounding tasks. However, the Kosmos-2 still fall short in handling broader aspect of visual tasks beyond region-level grounding.</p> <h2 id="method">Method</h2> <h3 id="pipeline">Pipeline</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/lion_overview-480.webp 480w,/assets/img/LION/lion_overview-800.webp 800w,/assets/img/LION/lion_overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/lion_overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Figure 4.</i> Overview of LION <a href="#1">[1]</a>. </p> <h3 id="spatial-visual-knowledge">Spatial Visual Knowledge</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/lion_spatial-480.webp 480w,/assets/img/LION/lion_spatial-800.webp 800w,/assets/img/LION/lion_spatial-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/lion_spatial.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Figure 5.</i> Representation of how LION handle spatial visual knowledge <a href="#1">[1]</a>. </p> <p>To incorporate spatial-aware visual knowledge into the model, the paper suggests reformatting the datasets into a unified format, that combines natural language descriptions and object coordinates enclosed in square brackets for instruction tuning, just like Kosmos-2 did.</p> <p>Still, it remains two main internal conflicts when the model tries to learn both image-level and region-level visual tasks. One is the need of region-level modality-alignment pre-training. This is because most MLLMs are only trained with global image features. And the second is the gap between the input-output modes of image-level and region level visual tasks. As mentioned earlier, the reformatted data contains text and coordinates, which can confuse the model when trained together.</p> <p>To address these two conflicts, a stage-wise instruction tuning strategy is applied, which is a three-stage training strategy.</p> <ol> <li> <p>Image-level For the first stage, the model learns general vision-language understanding, by fine-tuning the Q-Former and the image-level adapter in the LLM. This offers the model with image-level knowledge.</p> </li> <li> <p>Region-level And for the second stage, the model focuses on fine-grained spatial knowledge, by using a vision aggregator to capture detailed visual features along with the region-level adapter. At this stage, the model learns more about region-level knowledge, and as the model is trained on region-level AFTER the image-level, it can avoid severe interference between the two levels.</p> </li> <li> <p>Mixture of the both Finally at the third stage, the model combines the outputs from the previous two stages using a router, which dynamically balances image-level and region-level knowledge. The key component here is the router. The router not only balances between the two levels of knowledge, but also aligns the input-output mismatch by assigning adaptive weights to each adapters’ output based on the task type.</p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/stage-wise_instruction-tuning-480.webp 480w,/assets/img/LION/stage-wise_instruction-tuning-800.webp 800w,/assets/img/LION/stage-wise_instruction-tuning-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/stage-wise_instruction-tuning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Figure 6.</i> The stage-wise instruction-tuning strategy <a href="#1">[1]</a>. </p> \[\begin{align*} O^t = F(X) + \sum_{k=1}^{K=2}G_k^t \odot H_k(X), \end{align*}\] <p>where \(H_k(X)\) is an adapter for \(k\)-th adapter and \(F(X)\) is the output of FFN.</p> <p>To make the stage-wise instruction possible, we must first look at the placement of the adapters in the LLM. Each adapter is inserted at every Feed-Forward Network layer in a parallel manner within the frozen LLM. In this arrangement, the output features generated by the standard FFN are simply added to the output features generated by the adapter layer.</p> <p>Sine each adapter is trained separately, we treat these specialized components as distinct experts.</p> <p>However, as previously mentioned, the router is the key component that enables the model to use these adapters effectively. The router module dynamically decides how much to rely on each adapter based on the task by learning a weight vector G for every task.</p> <p>  For example, if the input involves spatial reasoning, the router increases the contribution of the region-level adapter by updating its weight, and vice versa for global reasoning.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/vision_aggregator-480.webp 480w,/assets/img/LION/vision_aggregator-800.webp 800w,/assets/img/LION/vision_aggregator-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/vision_aggregator.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Figure 7.</i> Vision Aggregator. </p> <p>The next challenge is to ensure that the model can capture sufficient visual details. To capture the fine-grained spatial-aware visual knowledge needed for tasks like visual grounding, the paper introduced a component called the Vision Aggregator.</p> <p>The Vision Aggregator functions as a tiny transformer-style network where that improves LION’s understanding of object boundaries, spatial relations, and fine object attributes.</p> <p>Ablation studies demonstrate that the VA promotes the extraction of this fine-grained knowledge and significantly improves referring expression comprehension (REC) performance.</p> <h3 id="soft-prompting">Soft Prompting</h3> <h4 id="image-tag-extraction">Image Tag Extraction</h4> <p>To improve LION’s capabilities, the paper included semantic comprehension. The authors used an off-the-shelf image tag extractor called Recognize Anything Model (RAM).</p> <h4 id="soft-prompting-1">Soft prompting</h4> <p>Since the predicted tags from this model are not flawless, they can mislead the model. So, LION uses a soft prompt to mitigate the influence of these imperfect tags. A trainable embedding is added to the instruction text that teaches the model how to use the tag information. In the paper, the phrase “According to <i>hint</i>, you are allowed to use or partially use the following tags: …”. This method helps guide the model to select valuable information from the tags and ignore the incorrect ones.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/soft-prompting-480.webp 480w,/assets/img/LION/soft-prompting-800.webp 800w,/assets/img/LION/soft-prompting-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/soft-prompting.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Figure 8.</i> Instruction template with soft prompt <a href="#1">[1]</a>. </p> <h2 id="experimental-results">Experimental Results</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/tab1-480.webp 480w,/assets/img/LION/tab1-800.webp 800w,/assets/img/LION/tab1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/tab1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Table 1.</i> Comparison on image vaptioning and Visual Question Answering (VQA). The best and second performances for each benchmark are indicated in bold and underline, respectively <a href="#1">[1]</a>. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/tab2-480.webp 480w,/assets/img/LION/tab2-800.webp 800w,/assets/img/LION/tab2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/tab2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Table 2.</i> Comparison on Referring Expression Comprehension (REC) <a href="#1">[1]</a>. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/tab3-480.webp 480w,/assets/img/LION/tab3-800.webp 800w,/assets/img/LION/tab3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/LION/tab3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center"> <i>Table 3.</i> Evaluation of object hallucination <a href="#1">[1]</a>. </p> <p>The LION architecture effectively addresses the challenge of insufficient visual knowledge extraction and reasoning, which affects existing Multimodal Large Language Models (MLLMs) that typically rely only on coarsely aligned image-text pairs.</p> <p>  The core innovation of LION is the injection of dual-level visual knowledge where, first, the Fine-grained spatial-aware knowledge is incorporated using the mixture-of-adapters using a router and a Vision Aggregator, and, second, the High-level semantic visual evidence is provided by image tags through a soft prompting method.</p> <h2 id="reference">Reference</h2> <p><a id="1" href="https://openaccess.thecvf.com/content/CVPR2024/liquid/Chen_LION_Empowering_Multimodal_Large_Language_Model_with_Dual-Level_Visual_Knowledge_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">[1]</a> Chen, G., Shen, L., Shao, R., Deng, X., &amp; Nie, L. (2024). Lion: Empowering multimodal large language model with dual-level visual knowledge. In <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> (pp. 26540-26550).</p> <p><a id="2" href="https://arxiv.org/abs/2306.14824" rel="external nofollow noopener" target="_blank">[1]</a> Peng, Z., Wang, W., Dong, L., Hao, Y ., Huang, S., Ma, S., &amp; Wei, F. (2023). Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/pk-yolo.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 WooSeok Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>