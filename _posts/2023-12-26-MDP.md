---
layout: post
title: Markov Decision Processes
date: 2023-12-26 20:00:00-0400
description: MDP and MRP
tags: RL
categories: concepts
giscus_comments: false
related_posts: false
related_publications: david-silver-lec2
---

# Markov Decision Processes

## 1. Markov Processes

### 1.1. Introduction to MDPs

- _Markov Decision Processes_ formally describe an environment for reinforcement learning.
- environment is fully observable

### 1.2. Markov Property

- the future is independent of the past given the present

> **Definition**<br>
> A state $$S_t$$ is _Markov_ if and only if
> $$\mathbb{P}[S_{t+1}|S_t] = \mathbb{P}[S_{t+1}|S_1, ..., S_t]$$

### 1.3. Markov Chains

> **Definition**<br>
> A _Markov Process_ (or _Markov Chain_) is tuple $$(\mathcal{S}, \mathcal{P})$$
> $$\mathcal{S}$$ is a finite set of states
> $$\mathcal{P}$$ is a state transition probability matrix

## 2. Markov Reward Processes

### 2.1. Markov Reward Process

> **Definition**<br>
> A _Markov Reward Process_ is a tuple $$(\mathcal{S}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma})$$

### 2.2. Return

> **Definition**<br> 
> $$G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}$$
> where $$\gamma \in [0, 1]$$.

### 2.3. Value Function

> **Definition**<br>
> $$v(s) = \mathbb{E}[G_t |S_t = s]$$
> _e.g.)_ $$G_1 = R_2 + \gamma R_3 + ... + \gamma^{T-2}R_T$$

### 2.4. Bellman Equation for MRPs

$$
\begin{align*}
    v(s)
    &= \mathbb{E}[G_t | S_t = s] \\
    &= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... | S_t = s] \\
    &= \mathbb{E}[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + ...) | S_t = s] \\
    &= \mathbb{E}[R_{t+1} + \gamma(G_{t+1}) | S_t = s] \\
    &= \mathbb{E}[R_{t+1} + \gamma(v(S_{t+1})) | S_t = s] \\
\end{align*}
$$

<p align="center">
    {% include figure.html path="../assets/img/mdp/value-function.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</p>

$$
v(s) = \mathcal{R}_s + \gamma\sum_{s'\in S}\mathcal{P}_{ss'}v(s')
$$

## 3. Markov Decision Processes

### 3.1. Markov Decision Process

> **Definition**<br>
> A _Markov Decision Process_ is a tuple $$(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$$

### 3.2. Policies

> **Definition**<br>
> $$\pi(a|s) = \mathbb{P}[A_t = a | S_t = s]$$

### 3.3. Value Function

> **Definition**<br>
> _state-value function_<br>
> $$v_\pi (s) = \mathbb{E}[G_t | S_t = s]$$

> **Definition**<br> 
> _action-value function_<br>
> $$q_\pi(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a]$$

### 3.4. Bellman Expectation Equation

$$
\begin{align*}
    v_\pi (s)
    &= \mathbb{E}[G_t | S_t = s] \\
    &= \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]
\end{align*}
$$

$$
\begin{align*}
    q_\pi(s, a)
    &= \mathbb{E}[G_t | S_t = s, A_t = a] \\
    &= \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
\end{align*}
$$

<p align="center">
    {% include figure.html path="../assets/img/mdp/state-value.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</p>

$$
\begin{align*}
    v_\pi(s) = \sum_{a\in \mathcal{A}}{\pi(a|s)q_\pi(s, a)}
\end{align*}
$$

<p align="center">
    {% include figure.html path="../assets/img/mdp/action-value.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</p>

$$
\begin{align*}
    q_\pi(s, a) = \mathcal{R}_{s}^{a} + \gamma\sum_{s'\in \mathcal{S}}{\mathcal{P}_{ss'}^{a}v_\pi(s')}
\end{align*}
$$

<p align="center">
    {% include figure.html path="../assets/img/mdp/combine.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</p>

$$
\begin{align*}
    v_\pi(s) = \sum_{a\in \mathcal{A}}{\pi(a|s)
    \left(
    \mathcal{R}_{s}^{a} + \gamma\sum_{s'\in \mathcal{S}}{\mathcal{P}_{ss'}^{a}v_\pi(s')}   \right)}
\end{align*}
$$
