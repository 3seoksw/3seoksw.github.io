<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://3seoksw.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://3seoksw.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-19T12:59:49+00:00</updated><id>https://3seoksw.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Markov Decision Processes</title><link href="https://3seoksw.github.io/blog/2023/MDP/" rel="alternate" type="text/html" title="Markov Decision Processes"/><published>2023-12-27T00:00:00+00:00</published><updated>2023-12-27T00:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/MDP</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/MDP/"><![CDATA[<h1 id="markov-decision-processes">Markov Decision Processes</h1> <h2 id="1-markov-processes">1. Markov Processes</h2> <h3 id="11-introduction-to-mdps">1.1. Introduction to MDPs</h3> <ul> <li><em>Markov Decision Processes</em> formally describe an environment for reinforcement learning.</li> <li>environment is fully observable</li> </ul> <h3 id="12-markov-property">1.2. Markov Property</h3> <ul> <li>the future is independent of the past given the present</li> </ul> <blockquote> <p><strong>Definition</strong><br/> A state \(S_t\) is <em>Markov</em> if and only if \(\mathbb{P}[S_{t+1}|S_t] = \mathbb{P}[S_{t+1}|S_1, ..., S_t]\)</p> </blockquote> <h3 id="13-markov-chains">1.3. Markov Chains</h3> <blockquote> <p><strong>Definition</strong><br/> A <em>Markov Process</em> (or <em>Markov Chain</em>) is tuple \((\mathcal{S}, \mathcal{P})\) \(\mathcal{S}\) is a finite set of states \(\mathcal{P}\) is a state transition probability matrix</p> </blockquote> <h2 id="2-markov-reward-processes">2. Markov Reward Processes</h2> <h3 id="21-markov-reward-process">2.1. Markov Reward Process</h3> <blockquote> <p><strong>Definition</strong><br/> A <em>Markov Reward Process</em> is a tuple \((\mathcal{S}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma})\)</p> </blockquote> <h3 id="22-return">2.2. Return</h3> <blockquote> <p><strong>Definition</strong><br/> \(G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\) where \(\gamma \in [0, 1]\).</p> </blockquote> <h3 id="23-value-function">2.3. Value Function</h3> <blockquote> <p><strong>Definition</strong><br/> \(v(s) = \mathbb{E}[G_t |S_t = s]\) <em>e.g.)</em> \(G_1 = R_2 + \gamma R_3 + ... + \gamma^{T-2}R_T\)</p> </blockquote> <h3 id="24-bellman-equation-for-mrps">2.4. Bellman Equation for MRPs</h3> \[\begin{align*} v(s) &amp;= \mathbb{E}[G_t | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + ...) | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma(G_{t+1}) | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma(v(S_{t+1})) | S_t = s] \\ \end{align*}\] <p align="center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mdp/value-function-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mdp/value-function-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mdp/value-function-1400.webp"/> <img src="/assets/img/mdp/value-function.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> \[v(s) = \mathcal{R}_s + \gamma\sum_{s'\in S}\mathcal{P}_{ss'}v(s')\] <h2 id="3-markov-decision-processes">3. Markov Decision Processes</h2> <h3 id="31-markov-decision-process">3.1. Markov Decision Process</h3> <blockquote> <p><strong>Definition</strong><br/> A <em>Markov Decision Process</em> is a tuple \((\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)\)</p> </blockquote> <h3 id="32-policies">3.2. Policies</h3> <blockquote> <p><strong>Definition</strong><br/> \(\pi(a|s) = \mathbb{P}[A_t = a | S_t = s]\)</p> </blockquote> <h3 id="33-value-function">3.3. Value Function</h3> <blockquote> <p><strong>Definition</strong><br/> <em>state-value function</em><br/> \(v_\pi (s) = \mathbb{E}[G_t | S_t = s]\)</p> </blockquote> <blockquote> <p><strong>Definition</strong><br/> <em>action-value function</em><br/> \(q_\pi(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a]\)</p> </blockquote> <h3 id="34-bellman-expectation-equation">3.4. Bellman Expectation Equation</h3> \[\begin{align*} v_\pi (s) &amp;= \mathbb{E}[G_t | S_t = s] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \end{align*}\] \[\begin{align*} q_\pi(s, a) &amp;= \mathbb{E}[G_t | S_t = s, A_t = a] \\ &amp;= \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \end{align*}\] <p align="center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mdp/state-value-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mdp/state-value-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mdp/state-value-1400.webp"/> <img src="/assets/img/mdp/state-value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> \[\begin{align*} v_\pi(s) = \sum_{a\in \mathcal{A}}{\pi(a|s)q_\pi(s, a)} \end{align*}\] <p align="center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mdp/action-value-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mdp/action-value-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mdp/action-value-1400.webp"/> <img src="/assets/img/mdp/action-value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> \[\begin{align*} q_\pi(s, a) = \mathcal{R}_{s}^{a} + \gamma\sum_{s'\in \mathcal{S}}{\mathcal{P}_{ss'}^{a}v_\pi(s')} \end{align*}\] <p align="center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mdp/combine-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mdp/combine-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mdp/combine-1400.webp"/> <img src="/assets/img/mdp/combine.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> \[\begin{align*} v_\pi(s) = \sum_{a\in \mathcal{A}}{\pi(a|s) \left( \mathcal{R}_{s}^{a} + \gamma\sum_{s'\in \mathcal{S}}{\mathcal{P}_{ss'}^{a}v_\pi(s')} \right)} \end{align*}\]]]></content><author><name></name></author><category term="concepts"/><category term="RL"/><summary type="html"><![CDATA[MDP and MRP]]></summary></entry><entry><title type="html">Evidence Lower Bound (ELBO)</title><link href="https://3seoksw.github.io/blog/2023/ELBO/" rel="alternate" type="text/html" title="Evidence Lower Bound (ELBO)"/><published>2023-11-08T02:00:00+00:00</published><updated>2023-11-08T02:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/ELBO</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/ELBO/"><![CDATA[<h1 id="elbo">ELBO</h1> <p>In <strong>Variational Bayesian Methods</strong>, the Evidence Lower Bound (<strong>ELBO</strong>) is a lower bound on the log-likelihood of some observed data.</p> <hr/> <h3 id="terminology-and-notation">Terminology and Notation</h3> <table> <tbody> <tr> <td>Let \(X\) and \(Z\) be random variables, jointly distributed with distribution \(p_\theta\). For example, \(p_\theta(X)\) is the <strong>Marginal Distribution</strong> of \(X\), and $$p_\theta(Z</td> <td>X)\(is the conditional distribution of\)Z\(given\)X\(. There, for any samle\)x \sim p_\theta\(, and any distribution\)q_\phi$$, we have</td> </tr> </tbody> </table> \[\ln{p_\theta}(x) \geq \mathbb{E}_{z\sim q_\phi}\left[\ln{\frac{p_\theta(x, z)}{q_\phi(z)}}\right].\] <p>LHS: <em>evidence</em> for \(x\) RHS: <em>evidence lower bound (ELBO)</em> for \(x\) The above is refered as the <em>ELBO inequality</em>.</p> <h3 id="applying">Applying</h3> <p>To derive the ELBO, we introduce <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s Inequality</a> applied to randam variables \(x \in X\) here:</p> \[\begin{align} f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)] \end{align}\] <p>We apply <em>Jensen’s Inequality</em> to the \(\log\) (marginal) probability of the observations to get the ELBO.</p> \[\begin{align} \log p(x) &amp;= \log\int_z{p(x, z)dz} \\ &amp;= \log\int_z{p(x, z)\frac{q(z)}{q(z)}dz} \\ &amp;= \log\int_z{\frac{p(x, z)}{q(z)}q(z)dz} \\ &amp;= \log\left({\mathbb{E}_{q(z)}\left[ {\frac{p(x, z)}{q(z)}}\right]}\right) \\ &amp;\geq \mathbb{E}_{q(z)}\left[ \log{\frac{p(x, z)}{q(z)}} \right] \\ &amp;= \mathbb{E}_{q(z)}\left[ \log{p(x, z)} \right] - \mathbb{E}_{q(z)}[\log{q(z)}] \end{align}\] <p>All together, the ELBO for a probability model \(p(x, z)\) and an approximation \(q(z)\) to the posterior is: \(\mathbb{E}_{q(z)}[\log{p(x, z)}]-\mathbb{E}_{q(z)}[\log{q(z)}]\)</p>]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[a lower bound on the log-likelihood]]></summary></entry><entry><title type="html">Kullback-Leibler Divergence (KLD)</title><link href="https://3seoksw.github.io/blog/2023/KLD/" rel="alternate" type="text/html" title="Kullback-Leibler Divergence (KLD)"/><published>2023-11-08T02:00:00+00:00</published><updated>2023-11-08T02:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/KLD</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/KLD/"><![CDATA[<h1 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h1> <p>Kullback-Leibler Divergence is a type of statistical distance: a measure of how one probability distribution \(P\) is different (or similar) from the other probability distribution \(Q\).</p> <h3 id="notation">Notation</h3> \[\begin{align*} D_{KL}(P || Q) \\ KL(P || Q) \end{align*}\] <h3 id="defintion">Defintion</h3> <p>For discrete probability distributions \(P\) and \(Q\) defined on the same sample space \(\mathcal{X}\), the relative entropy from \(Q\) to \(P\) is defined to be \(KL(P || Q) = \sum_{x\in\mathcal{X}}P(x)\log{\frac{P(x)}{Q(x)}}\) For distributions \(P\) and \(Q\) of a continuous random variable, the relative entropy is defined to be \(KL(P || Q) = \int_{-\infty}^{+\infty}p(x)\log{\frac{p(x)}{q(x)}}dx\)</p> <h3 id="applying-kullback-leibler-divergence-to-bayesian-backpropagation">Applying Kullback-Leibler Divergence to Bayesian Backpropagation</h3> \[\begin{align*} \theta^{*} &amp;= \text{argmin}_{\theta}KL[q(w|\theta) \; || \; P(w|\mathcal{D})] \\ &amp;= \text{argmin}_{\theta}\int{q(w|\theta)\log{\frac{q(w|\theta)}{P(w|\mathcal{D})}}}dw \\ &amp;= \text{argmin}_{\theta}\int{q(w|\theta)\log{\frac{q(w|\theta)P(\mathcal{D})}{P(w)P(\mathcal{D}|w)}}}dw \\ &amp;= \text{argmin}_{\theta}\int{q(w|\theta)\log{\frac{q(w|\theta)}{P(w)P(\mathcal{D}|w)}}}dw \\ &amp;= \text{argmin}_{\theta}\left(\int{q(w|\theta)\log{\frac{q(w|\theta)}{P(w)}}}dw \; - \; \int{q(w|\mathcal{D})\log{P(\mathcal{D}|w)}dw} \right) \\ &amp;= KL[q(w|\theta) \; || \; P(w)] \; - \; \mathbb{E}_{q(w|\mathcal{D})}[\log{P(\mathcal{D}|w)}] \end{align*}\] <p>Complexity cost (prior-dependent part): \(KL[q(w|\theta) \mid \mid P(w)]\) Likelihood cost (data-dependent part): \(\mathbb{E}_{q(w \mid \mathcal{D})}[P(\mathcal{D}\mid w)]\)</p> <p>Resulting cost function:</p> \[\mathcal{F}(\mathcal{D}, \theta) = KL[q(w|\theta) || P(w)] - \mathbb{E}_{q(w|\mathcal{D})}[\log P(\mathcal{D}|w)]\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[statistical distance between two distributions]]></summary></entry><entry><title type="html">Maximum Likelihood Estimation (MLE)</title><link href="https://3seoksw.github.io/blog/2023/MLE/" rel="alternate" type="text/html" title="Maximum Likelihood Estimation (MLE)"/><published>2023-09-15T01:30:00+00:00</published><updated>2023-09-15T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/MLE</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/MLE/"><![CDATA[<h1 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h1> <p>A maximum likelihood estimation (MLE) is a method of estimating the parameters of the given likelihood probability distribution.</p> <h3 id="definition">Definition</h3> <p>A value of $\theta$ that maximizes \(L(\theta|x_1, x_2, ..., x_n)\). Most likely, natural log will be plugged (<a href="https://3seoksw.github.io/blog/2023/likelihood">Log-Likelihood Function</a>).</p> \[\begin{align*} \theta^* &amp;= \text{argmax}_\theta l(\theta|x_1, x_2, ..., x_n) \\ &amp;= \text{argmax}_\theta log(\mathcal{L}(\theta|x_1, x_2, ..., x_n)) \\ &amp;= \text{argmax}_\theta log(\prod_{i=1}^{n}f(x_i|\theta)) \\ &amp;= \text{argmax}_\theta log(f(x_1|\theta) \times f(x_2|\theta) \times ... \times f(x_n|\theta)) \\ &amp;= \text{argmax}_\theta \sum_{i=1}^{n}log(f(x_i|\theta)) \end{align*}\] <p>From <strong>Bayesian Backpropagation</strong>:</p> \[\begin{align*} w^\text{MLE} &amp;= \text{argmax}_wl(w|\mathcal{D}) \\ &amp;= \text{argmax}_w\log{\mathcal{L}(w|\mathcal{D})} \\ &amp;= \text{argmax}_w\log{P(\mathcal{D}|w)} \\ &amp;= \text{argmax}_w\log{P(\mathcal{D_1}, ...\mathcal{D_n}|w)} \\ &amp;= \text{argmax}_w\log{\prod_{i=1}^{n}P(\mathcal{D_i}|w)} \\ &amp;= \text{argmax}_w\sum_{i=1}^{n}{\log{P(\mathcal{D_i}|w)}} \\ &amp;= \text{argmax}_w\sum_{i=1}^{n}{\log{P(y_i|x_i,w)}} \\ \end{align*}\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[estimation method of a likelihood distribution]]></summary></entry><entry><title type="html">Bayes’ Theorem</title><link href="https://3seoksw.github.io/blog/2023/bayes-theorem/" rel="alternate" type="text/html" title="Bayes’ Theorem"/><published>2023-09-14T01:30:00+00:00</published><updated>2023-09-14T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/bayes-theorem</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/bayes-theorem/"><![CDATA[<h1 id="bayes-theorem">Bayes’ Theorem</h1> <p><strong>Bayes’ Theorem</strong> (or Bayesian Theorem) is a statistical method to update our prior beliefs. Bayes theorem can be used in various fields such as in Machine Learning (ML) method.</p> <hr/> <h3 id="definition">Definition</h3> \[\begin{align*} \text{Posterior} \propto \text{Prior} \times \text{Likelihood} \\ P(A|B) = \frac{P(B|A) \times P(A)}{P(B)} \\ \end{align*}\] <h3 id="usage-in-ml">Usage in ML</h3> \[\begin{align*} P(w|\mathcal{D}) \propto P(\mathcal{D}|w) \times P(w) \\ \end{align*}\] <p>\(w\): Prior weights for a neural network<br/> \(\mathcal{D}\): Data<br/> \(P(w|\mathcal{D})\): Posterior distribution, a probability distribution of the neural network weights \(w\) after observing the data \(\mathcal{D}\)<br/> \(P(\mathcal{D}|w)\): Likelihood function, represents how well the neural network with parameters \(w\) fits the observed data \(\mathcal{D}\)<br/> \(P(w)\): Prior, prior beliefs about the neural network weights before observing the data \(\mathcal{D}\)<br/> \(P(y^*|x^*)=\mathbb{E}_{P(w|\mathcal{D})}[P(y^*|x^*, w)]\) At prediction time, the predictive distribution over the target \(y^*\) given a test input \(x^*\)<br/></p>]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[Bayes Rule, probability of an event based on prior knowledge]]></summary></entry><entry><title type="html">Likelihood Function</title><link href="https://3seoksw.github.io/blog/2023/likelihood/" rel="alternate" type="text/html" title="Likelihood Function"/><published>2023-09-14T01:30:00+00:00</published><updated>2023-09-14T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/likelihood</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/likelihood/"><![CDATA[<h1 id="likelihood-function">Likelihood Function</h1> <p>The likelihood function is the joint probability of the given data (say \(x\)) viewed as a function.</p> <h3 id="definition">Definition</h3> \[\begin{align*} &amp; L(\theta|x_1, x_2, ..., x_n) \\ &amp;= \text{joint pmf/pdf of random variables } x_1, x_2, ..., x_n \text{ from } \theta \\ &amp;= f(x_1, x_2, ..., x_n|\theta) \\ &amp;= f(x_1|\theta) \times f(x_2|\theta) \times ... \times f(x_n|\theta) \\ &amp;= \prod_{i=1}^{n}f(x_i|\theta) \\ \end{align*}\] <h3 id="log-likelihood-function">Log-Likelihood Function</h3> <p>Plugging the <strong>Likelihood function</strong> into a logarithm shows as follows:</p> \[\begin{align*} &amp;l(\theta|x_1, x_2, ..., x_n) \\ &amp;= log(L(\theta|x_1, x_2, ..., x_n)) \\ &amp;= log(\prod_{i=1}^{n}f(x_i|\theta)) \\ &amp;= \sum_{i=1}^{n}{log{f(x_i|\theta)}} \end{align*}\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[likelihood function]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>