<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://3seoksw.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://3seoksw.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-27T14:13:48+00:00</updated><id>https://3seoksw.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Evidence Lower Bound (ELBO)</title><link href="https://3seoksw.github.io/blog/2023/ELBO/" rel="alternate" type="text/html" title="Evidence Lower Bound (ELBO)"/><published>2023-11-08T02:00:00+00:00</published><updated>2023-11-08T02:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/ELBO</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/ELBO/"><![CDATA[<h1 id="elbo">ELBO</h1> <p>In <strong>Variational Bayesian Methods</strong>, the Evidence Lower Bound (<strong>ELBO</strong>) is a lower bound on the log-likelihood of some observed data.</p> <hr/> <h3 id="terminology-and-notation">Terminology and Notation</h3> <table> <tbody> <tr> <td>Let \(X\) and \(Z\) be random variables, jointly distributed with distribution \(p_\theta\). For example, \(p_\theta(X)\) is the <strong>Marginal Distribution</strong> of \(X\), and $$p_\theta(Z</td> <td>X)\(is the conditional distribution of\)Z\(given\)X\(. There, for any samle\)x \sim p_\theta\(, and any distribution\)q_\phi$$, we have</td> </tr> </tbody> </table> \[\ln{p_\theta}(x) \geq \mathbb{E}_{z\sim q_\phi}\left[\ln{\frac{p_\theta(x, z)}{q_\phi(z)}}\right].\] <p>LHS: <em>evidence</em> for \(x\) RHS: <em>evidence lower bound (ELBO)</em> for \(x\) The above is refered as the <em>ELBO inequality</em>.</p> <h3 id="applying">Applying</h3> <p>To derive the ELBO, we introduce <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s Inequality</a> applied to randam variables \(x \in X\) here:</p> \[\begin{align} f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)] \end{align}\] <p>We apply <em>Jensen’s Inequality</em> to the \(\log\) (marginal) probability of the observations to get the ELBO.</p> \[\begin{align} \log p(x) &amp;= \log\int_z{p(x, z)dz} \\ &amp;= \log\int_z{p(x, z)\frac{q(z)}{q(z)}dz} \\ &amp;= \log\int_z{\frac{p(x, z)}{q(z)}q(z)dz} \\ &amp;= \log\left({\mathbb{E}_{q(z)}\left[ {\frac{p(x, z)}{q(z)}}\right]}\right) \\ &amp;\geq \mathbb{E}_{q(z)}\left[ \log{\frac{p(x, z)}{q(z)}} \right] \\ &amp;= \mathbb{E}_{q(z)}\left[ \log{p(x, z)} \right] - \mathbb{E}_{q(z)}[\log{q(z)}] \end{align}\] <p>All together, the ELBO for a probability model \(p(x, z)\) and an approximation \(q(z)\) to the posterior is: \(\mathbb{E}_{q(z)}[\log{p(x, z)}]-\mathbb{E}_{q(z)}[\log{q(z)}]\)</p>]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[a lower bound on the log-likelihood]]></summary></entry><entry><title type="html">Kullback-Leibler Divergence (KLD)</title><link href="https://3seoksw.github.io/blog/2023/KLD/" rel="alternate" type="text/html" title="Kullback-Leibler Divergence (KLD)"/><published>2023-11-08T02:00:00+00:00</published><updated>2023-11-08T02:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/KLD</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/KLD/"><![CDATA[<h1 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h1> <p>Kullback-Leibler Divergence is a type of statistical distance: a measure of how one probability distribution \(P\) is different (or similar) from the other probability distribution \(Q\).</p> <h3 id="notation">Notation</h3> \[\begin{align*} D_{KL}(P || Q) \\ KL(P || Q) \end{align*}\] <h3 id="defintion">Defintion</h3> <p>For discrete probability distributions \(P\) and \(Q\) defined on the same sample space \(\mathcal{X}\), the relative entropy from \(Q\) to \(P\) is defined to be \(KL(P || Q) = \sum_{x\in\mathcal{X}}P(x)\log{\frac{P(x)}{Q(x)}}\) For distributions \(P\) and \(Q\) of a continuous random variable, the relative entropy is defined to be \(KL(P || Q) = \int_{-\infty}^{+\infty}p(x)\log{\frac{p(x)}{q(x)}}dx\)</p> <h3 id="applying-kullback-leibler-divergence-to-bayesian-backpropagation">Applying Kullback-Leibler Divergence to Bayesian Backpropagation</h3> \[\begin{align*} \theta^{*} &amp;= \text{argmin}_{\theta}KL[q(w|\theta) \; || \; P(w|\mathcal{D})] \\ &amp;= \text{argmin}_{\theta}\int{q(w|\theta)\log{\frac{q(w|\theta)}{P(w|\mathcal{D})}}}dw \\ &amp;= \text{argmin}_{\theta}\int{q(w|\theta)\log{\frac{q(w|\theta)P(\mathcal{D})}{P(w)P(\mathcal{D}|w)}}}dw \\ &amp;= \text{argmin}_{\theta}\int{q(w|\theta)\log{\frac{q(w|\theta)}{P(w)P(\mathcal{D}|w)}}}dw \\ &amp;= \text{argmin}_{\theta}\left(\int{q(w|\theta)\log{\frac{q(w|\theta)}{P(w)}}}dw \; - \; \int{q(w|\mathcal{D})\log{P(\mathcal{D}|w)}dw} \right) \\ &amp;= KL[q(w|\theta) \; || \; P(w)] \; - \; \mathbb{E}_{q(w|\mathcal{D})}[\log{P(\mathcal{D}|w)}] \end{align*}\] <table> <tbody> <tr> <td>Complexity cost (prior-dependent part): $$KL[q(w</td> <td>\theta)</td> <td> </td> <td>P(w)]$$</td> </tr> <tr> <td>Likelihood cost (data-dependent part): $$\mathbb{E}_{q(w</td> <td>\mathcal{D})}[P(\mathcal{D}</td> <td>w)]$$</td> <td> </td> </tr> </tbody> </table> <p>Resulting cost function:</p> \[\mathcal{F}(\mathcal{D}, \theta) = KL[q(w|\theta) || P(w)] - \mathbb{E}_{q(w|\mathcal{D})}[\log P(\mathcal{D}|w)]\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[statistical distance between two distributions]]></summary></entry><entry><title type="html">Maximum Likelihood Estimation (MLE)</title><link href="https://3seoksw.github.io/blog/2023/MLE/" rel="alternate" type="text/html" title="Maximum Likelihood Estimation (MLE)"/><published>2023-09-15T01:30:00+00:00</published><updated>2023-09-15T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/MLE</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/MLE/"><![CDATA[<h1 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h1> <p>A maximum likelihood estimation (MLE) is a method of estimating the parameters of the given likelihood probability distribution.</p> <h3 id="definition">Definition</h3> <p>A value of $\theta$ that maximizes \(L(\theta|x_1, x_2, ..., x_n)\). Most likely, natural log will be plugged (<a href="https://3seoksw.github.io/blog/2023/likelihood">Log-Likelihood Function</a>).</p> \[\begin{align*} \theta^* &amp;= \text{argmax}_\theta l(\theta|x_1, x_2, ..., x_n) \\ &amp;= \text{argmax}_\theta log(\mathcal{L}(\theta|x_1, x_2, ..., x_n)) \\ &amp;= \text{argmax}_\theta log(\prod_{i=1}^{n}f(x_i|\theta)) \\ &amp;= \text{argmax}_\theta log(f(x_1|\theta) \times f(x_2|\theta) \times ... \times f(x_n|\theta)) \\ &amp;= \text{argmax}_\theta \sum_{i=1}^{n}log(f(x_i|\theta)) \end{align*}\] <p>From <strong>Bayesian Backpropagation</strong>:</p> \[\begin{align*} w^\text{MLE} &amp;= \text{argmax}_wl(w|\mathcal{D}) \\ &amp;= \text{argmax}_w\log{\mathcal{L}(w|\mathcal{D})} \\ &amp;= \text{argmax}_w\log{P(\mathcal{D}|w)} \\ &amp;= \text{argmax}_w\log{P(\mathcal{D_1}, ...\mathcal{D_n}|w)} \\ &amp;= \text{argmax}_w\log{\prod_{i=1}^{n}P(\mathcal{D_i}|w)} \\ &amp;= \text{argmax}_w\sum_{i=1}^{n}{\log{P(\mathcal{D_i}|w)}} \\ &amp;= \text{argmax}_w\sum_{i=1}^{n}{\log{P(y_i|x_i,w)}} \\ \end{align*}\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[estimation method of a likelihood distribution]]></summary></entry><entry><title type="html">Bayes’ Theorem</title><link href="https://3seoksw.github.io/blog/2023/bayes-theorem/" rel="alternate" type="text/html" title="Bayes’ Theorem"/><published>2023-09-14T01:30:00+00:00</published><updated>2023-09-14T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/bayes-theorem</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/bayes-theorem/"><![CDATA[<h1 id="bayes-theorem">Bayes’ Theorem</h1> <p><strong>Bayes’ Theorem</strong> (or Bayesian Theorem) is a statistical method to update our prior beliefs. Bayes theorem can be used in various fields such as in Machine Learning (ML) method.</p> <hr/> <h3 id="definition">Definition</h3> \[\begin{align*} \text{Posterior} \propto \text{Prior} \times \text{Likelihood} \\ P(A|B) = \frac{P(B|A) \times P(A)}{P(B)} \\ \end{align*}\] <h3 id="usage-in-ml">Usage in ML</h3> \[\begin{align*} P(w|\mathcal{D}) \propto P(\mathcal{D}|w) \times P(w) \\ \end{align*}\] <p>\(w\): Prior weights for a neural network<br/> \(\mathcal{D}\): Data<br/> \(P(w|\mathcal{D})\): Posterior distribution, a probability distribution of the neural network weights \(w\) after observing the data \(\mathcal{D}\)<br/> \(P(\mathcal{D}|w)\): Likelihood function, represents how well the neural network with parameters \(w\) fits the observed data \(\mathcal{D}\)<br/> \(P(w)\): Prior, prior beliefs about the neural network weights before observing the data \(\mathcal{D}\)<br/> \(P(y^*|x^*)=\mathbb{E}_{P(w|\mathcal{D})}[P(y^*|x^*, w)]\) At prediction time, the predictive distribution over the target \(y^*\) given a test input \(x^*\)<br/></p>]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[Bayes Rule, probability of an event based on prior knowledge]]></summary></entry><entry><title type="html">Likelihood Function</title><link href="https://3seoksw.github.io/blog/2023/likelihood/" rel="alternate" type="text/html" title="Likelihood Function"/><published>2023-09-14T01:30:00+00:00</published><updated>2023-09-14T01:30:00+00:00</updated><id>https://3seoksw.github.io/blog/2023/likelihood</id><content type="html" xml:base="https://3seoksw.github.io/blog/2023/likelihood/"><![CDATA[<h1 id="likelihood-function">Likelihood Function</h1> <p>The likelihood function is the joint probability of the given data (say \(x\)) viewed as a function.</p> <h3 id="definition">Definition</h3> \[\begin{align*} &amp; L(\theta|x_1, x_2, ..., x_n) \\ &amp;= \text{joint pmf/pdf of random variables } x_1, x_2, ..., x_n \text{ from } \theta \\ &amp;= f(x_1, x_2, ..., x_n|\theta) \\ &amp;= f(x_1|\theta) \times f(x_2|\theta) \times ... \times f(x_n|\theta) \\ &amp;= \prod_{i=1}^{n}f(x_i|\theta) \\ \end{align*}\] <h3 id="log-likelihood-function">Log-Likelihood Function</h3> <p>Plugging the <strong>Likelihood function</strong> into a logarithm shows as follows:</p> \[\begin{align*} &amp;l(\theta|x_1, x_2, ..., x_n) \\ &amp;= log(L(\theta|x_1, x_2, ..., x_n)) \\ &amp;= log(\prod_{i=1}^{n}f(x_i|\theta)) \\ &amp;= \sum_{i=1}^{n}{log{f(x_i|\theta)}} \end{align*}\]]]></content><author><name></name></author><category term="concepts"/><category term="statistics"/><category term="bayes"/><summary type="html"><![CDATA[likelihood function]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>