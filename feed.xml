<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://3seoksw.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://3seoksw.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-23T03:49:28+00:00</updated><id>https://3seoksw.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">COLLABLLM, From Passive Responders to Active Collaborators</title><link href="https://3seoksw.github.io/blog/2025/COLLABLLM-review/" rel="alternate" type="text/html" title="COLLABLLM, From Passive Responders to Active Collaborators"/><published>2025-11-12T00:00:00+00:00</published><updated>2025-11-12T00:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2025/COLLABLLM-review</id><content type="html" xml:base="https://3seoksw.github.io/blog/2025/COLLABLLM-review/"><![CDATA[<p>The review is done with the following paper and the figures used for this article are derived from the paper:<br/> <a href="https://arxiv.org/abs/2502.00640">COLLABLLM: From Passive Responders to Active Collaborators</a> <d-cite key="collabllm"></d-cite>.</p> <h2 id="abstract">Abstract</h2> <p>COLLABLLM’s motivation and its idea are straightforward and intuitive.</p> <p>I believe you might also have experienced this. Imagin you’re using a large language model like Chat-GPT, and you ask some question to the model then you’ve probably gotten a passive answer at first. Then you had to refine your instruction since the model’s initial answer wasn’t satisfactory and repeat this process multiple times, which is quite frustrating experience and consumes a lot of time. This largely comes from how most models are trained and optimized for single-turn helpfulness, which encourages passive responses and ultimately limits long-term interaction. COLLABLLM aims for a more interactive, multiturn-based experience. To address the aforementioned problems, COLLABLLM introduces two ideas which are Collaborative simulation and a Multiturn-aware Reward which is called MR. Together, these make the model more proactive and lead to higher task performance and better interactivity across multiple turns.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/comp_collabllm-480.webp 480w,/assets/img/COLLABLLM/comp_collabllm-800.webp 800w,/assets/img/COLLABLLM/comp_collabllm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/comp_collabllm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1. Comparisons between existing LLMs and COLLABLLM. <d-cite key="collabllm"></d-cite> </div> <h2 id="overview-of-collabllm">Overview of COLLABLLM</h2> <p>The following figure shows the overall process of how COLLABLLM works.</p> <p>For instance, the user gives an instruction to the model asking to write about how optimism can improve our well-being. Given the context state x, the model outputs a response y from the policy pi. What differs from other models is that instead of giving the answer right away, it asks more contextual questions like what kind of tone are you aiming for.</p> <p>The way model is not giving you passive answer, rather asking contextual questions is possible because the model is using multiturn-aware reward function to ensure model is accounting for multiturn. Also, making the model be aware of future turns is done by using collaborative simulation to see what would the future conversation be like.</p> <p>Collaborative simulation is to sample future conversations given the context state. You can think of collaborative simulation as a conversation lookahead method between user and the model. However, it is impossible to know what would users ask in near future, and even corresponding replies to unknown user’s future input.</p> <p>To make this feasible for enabling lookahead of conversations, a simulated user such as GPT 4o that imitates the actual user generates user’s future input. In collaborative simulation, forward sampling is used to retrieve unknown future dialogue between the user and the model. Ultimately, this enables the model take future conversation into account and choose responses aligned with a long-term goal, instead of a current-focused goal.</p> <p>Finally, they apply reinforcement learning fine-tuning such as SFT, PPO, and DPO using the MR function.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/framework-480.webp 480w,/assets/img/COLLABLLM/framework-800.webp 800w,/assets/img/COLLABLLM/framework-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/framework.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2. COLLABLLM Framework. </div> <h2 id="problem-formulation">Problem Formulation</h2> <ul> <li>Multiple Turns</li> </ul> \[\begin{align*} &amp; t_j = \{u_j, m_j \}, \text{ at turn } j=1, \cdots, K \\ &amp; u_j: \text{ user input} \\ &amp; m_j: \text{ model output} \\ &amp; t_{1:j} = \{ t_1, \cdots, t_j \} \\ &amp; t_{j}^{h} = t_{1:j} \cup \{u_j\} \end{align*}\] <ul> <li>Multiturn-aware Reward (MR)</li> </ul> \[\begin{align*} &amp; R^{*}(t_{1:K}|g): \text{ objective given some ground-truth goal} \\ &amp; MR(m_j | t_j^{h}, g) = \mathbb{E}_{t_j^f\sim P(t_j^f | t_{1:j})}[R^{*}(t_{1:j}\cup t_j^h | g)] \\ &amp; t_j^f = t_{j+1:K}: \text{ forward trajectory following } j\text{-th turn} \end{align*}\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/MR_simulation-480.webp 480w,/assets/img/COLLABLLM/MR_simulation-800.webp 800w,/assets/img/COLLABLLM/MR_simulation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/MR_simulation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3. Multiturn-aware Rewards from collaborative simulation. </div> <h3 id="conversation-level-reward">Conversation-level Reward</h3> \[R^{*}(t|g) = R_{\text{ext}}(t, g) + R_{\text{int}}(t)\] <ul> <li>Extrinsic Reward <ul> <li>\(R_{\text{ext}}(t, g) = S(\text{Extract}(t), y_g)\) evaluates how well the conversation achieves the user’s goal \(g\)</li> <li>\(S(\cdot, \cdot)\) evaluates task-specific metrics (<em>e.g.</em>, accuracy or similarity)</li> <li>\(\text{Extract(t)}\) extracts the final response (solution) from the conversation \(t\)</li> </ul> </li> <li>Intrinsic Reward <ul> <li>\(R_{\text{int}}(t) = - min[\lambda \cdot \text{TokenCount}(t), 1] + R_{LLM}(t)\), comprises penalty and helpfulness</li> <li>\(\lambda\) controls the penalty for the number of tokens being used</li> <li>\(R_{LLM}\) evaluates user-valued objectives (<em>e.g.</em>, engagement or interactivity)</li> </ul> </li> </ul> <p>The above uses LLM as a judge in terms of evaluating the intrinsic variables <d-cite key="zheng2023judging"><d-cite>.</d-cite></d-cite></p> <h3 id="forward-sampling">Forward Sampling</h3> \[t_j^f \sim P(t_j^f | t_{1:j})\] <ul> <li>User Simulator <ul> <li>Simulator \(U\) generates a probabilistic distribution \(P(u \mid t)\)</li> </ul> </li> <li>Sampling Method <ul> <li>Naive approach is to use Monte Carlo Sampling \(\rightarrow\) computationally expensive</li> <li>Introduce a window size \(w\) (trade objectives for huge cost savings)</li> </ul> </li> </ul> \[t_j^{f_w} = t_{j+1:j+w} \leftrightarrow t_j^f = t_{j+1:K}\] <h2 id="experiments">Experiments</h2> <h3 id="experimental-setup">Experimental Setup</h3> <p>COLLABLLM is based on Llama 3.1, and it has four variants. First two are offline models, supervised fine-tuning model and offline DPO model. Offline models only use the pre-determined datasets to update the model’s policy network during training. Then from the first two models, these two are further trained to online models, PPO and online DPO model. Online models are participating in the simulation to compute new MRs and update the policy network during training.</p> <p>So the difference between offline models and online models is that offline models do not participate in the simulation and online models do.</p> <p>Two baseline models will be used in the paper. One is called based model which is vanila Llama-3.1. The second one is called proactive base model, and it is a base model with proactive prompt engineering model. Proactive base model is simply base model given with the prompt as such figure 4 to be more collaborative and interactive.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/MR-480.webp 480w,/assets/img/COLLABLLM/MR-800.webp 800w,/assets/img/COLLABLLM/MR-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/MR.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 4. Generating high-quality conversation data with Multiturn-aware Rewards (MR). </div> <p>And COLLABLLM is evaluated with the baseline models in three different environment datasets.</p> <p>First is MediumDocEdit-Chat dataset focusing on document editing sampled from Medium articles. It is evaluated with BLEU score measuring similarity between the extracted document and the original article.</p> <p>Second is BigCodeBench-Chat dataset meant for coding assistance. It is sampled from BigCodeBench dataset and is using pass rate as an evaluation metric.</p> <p>Finally, MATH-chat dataset is used and it’s sampled from MATH dataset. The task is evaluated with the accuracy metric.</p> <p>In addition to the task-specific metrics, two task-agnostic metrics are incorporated, one is average token count and the other is interactivity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/simulated_env-480.webp 480w,/assets/img/COLLABLLM/simulated_env-800.webp 800w,/assets/img/COLLABLLM/simulated_env-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/simulated_env.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 5. Simulated Multiturn Environment Datasets. </div> <h3 id="results-of-simulated-experiments">Results of Simulated Experiments</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/tab1-480.webp 480w,/assets/img/COLLABLLM/tab1-800.webp 800w,/assets/img/COLLABLLM/tab1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/tab1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Table 1. Evaluation results on our multiturn datasets. Green zone: Baselines; Orange zone: Variants of COLLABLLMs. Rel. Improv. indicates the relative improvements of COLLABLLMs trained with Online DPO over Proactive Base. </div> <h3 id="ablation-study-on-reward-mechanism">Ablation Study on Reward Mechanism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/ablation-480.webp 480w,/assets/img/COLLABLLM/ablation-800.webp 800w,/assets/img/COLLABLLM/ablation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/ablation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 6. Ablation Study of Reward Mechanisms on MediumDocEdit-Chat. This figure compares three immediate reward mechanisms with three MR variants. </div> <h3 id="real-world-user-study">Real-world User Study</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/COLLABLLM/real-world-480.webp 480w,/assets/img/COLLABLLM/real-world-800.webp 800w,/assets/img/COLLABLLM/real-world-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/COLLABLLM/real-world.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 7. Real-world user study includes 201 participants interacting with Base, Proactive Base, and COLLABLLM. (a) document quality (b) overall interaction experience (c) spent time (d) additional assessments every three turns. </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Most LLMs make passive and short-sighted output due to single-turn training</li> <li>Add a future lookahead</li> <li>COLLABLLM introduces collaborative simulator and multiturn-aware reward (MR) \(\rightarrow\) Shows effectiveness, efficiency, engagement throughout extensive simulated and real-world evaluations.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="LLM"/><summary type="html"><![CDATA[Multiturn-aware LLM aiming for long-term goal]]></summary></entry><entry><title type="html">LION, Empowering MLLM with Dual-Level Visual Knowledge</title><link href="https://3seoksw.github.io/blog/2025/LION-review/" rel="alternate" type="text/html" title="LION, Empowering MLLM with Dual-Level Visual Knowledge"/><published>2025-10-09T00:00:00+00:00</published><updated>2025-10-09T00:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2025/LION-review</id><content type="html" xml:base="https://3seoksw.github.io/blog/2025/LION-review/"><![CDATA[<p>The review is done with the following paper:<br/> <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Chen_LION_Empowering_Multimodal_Large_Language_Model_with_Dual-Level_Visual_Knowledge_CVPR_2024_paper.html">LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge</a> <d-cite key="lion"></d-cite>.</p> <h2 id="abstract">Abstract</h2> <p>Existing MLLMs mainly use vision encoders that are pre-trained on coarsely aligned image-text pairs, which often leads to vague and inaccurate responses. These issues are due to the insufficient extraction and reasoning of visual knowledge. In other words, the existing models struggle from region-level tasks.</p> <p>To tackle this problem, the paper proposes the LION model. The objective of the model is to inject two levels of visual knowledge, which are image-level and region-level understanding. To make this possible, the model incorporates fine-grained spatial-aware knowledge, and applies soft prompting of high-level semantic visual evidence. These two enable the MLLM  to capture both global and local visual information from a given image.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/LION_MLLM_comp-480.webp 480w,/assets/img/LION/LION_MLLM_comp-800.webp 800w,/assets/img/LION/LION_MLLM_comp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/LION_MLLM_comp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 1.</i> Comparison between existing MLLMs and LION <a href="#1">[1]</a>. </p> <h2 id="related-works--visual-grounding">Related Works – Visual Grounding</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/bbox_rep-480.webp 480w,/assets/img/LION/bbox_rep-800.webp 800w,/assets/img/LION/bbox_rep-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/bbox_rep.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 2.</i> Representation of object description and bounding box which follows Markdown link format <a href="#2">[1]</a>. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/kosmos-2-480.webp 480w,/assets/img/LION/kosmos-2-800.webp 800w,/assets/img/LION/kosmos-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/kosmos-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure 3. Kosmos-2 offering object description with bounding box <d-cite key="peng2023kosmos"></d-cite>.</p> <p>While there are numerous works on assigning visual grounding tasks to MLLMs, Kosmos-2 is a great example for comparison with the LION model. Kosmos-2 converts existing datasets into a Markdown-style link format. This format represents a bounding box which includes spatial coordinates enclosed in square brackets as shown in Figure 2 and 3.</p> <p>The reason this Markdown link format matters is that it provides a tokenizer-friendly representation, making it easier for the model to understand both the semantic meaning of the image tags and their spatial locations. In this way, it effectively bridges the gap between text and visual grounding tasks. However, the Kosmos-2 still fall short in handling broader aspect of visual tasks beyond region-level grounding.</p> <h2 id="method">Method</h2> <h3 id="pipeline">Pipeline</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/lion_overview-480.webp 480w,/assets/img/LION/lion_overview-800.webp 800w,/assets/img/LION/lion_overview-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/lion_overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 4.</i> Overview of LION <a href="#1">[1]</a>. </p> <h3 id="spatial-visual-knowledge">Spatial Visual Knowledge</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/lion_spatial-480.webp 480w,/assets/img/LION/lion_spatial-800.webp 800w,/assets/img/LION/lion_spatial-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/lion_spatial.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 5.</i> Representation of how LION handle spatial visual knowledge <a href="#1">[1]</a>. </p> <p>To incorporate spatial-aware visual knowledge into the model, the paper suggests reformatting the datasets into a unified format, that combines natural language descriptions and object coordinates enclosed in square brackets for instruction tuning, just like Kosmos-2 did.</p> <p>Still, it remains two main internal conflicts when the model tries to learn both image-level and region-level visual tasks. One is the need of region-level modality-alignment pre-training. This is because most MLLMs are only trained with global image features. And the second is the gap between the input-output modes of image-level and region level visual tasks. As mentioned earlier, the reformatted data contains text and coordinates, which can confuse the model when trained together.</p> <p>To address these two conflicts, a stage-wise instruction tuning strategy is applied, which is a three-stage training strategy.</p> <ol> <li> <p>Image-level For the first stage, the model learns general vision-language understanding, by fine-tuning the Q-Former and the image-level adapter in the LLM. This offers the model with image-level knowledge.</p> </li> <li> <p>Region-level And for the second stage, the model focuses on fine-grained spatial knowledge, by using a vision aggregator to capture detailed visual features along with the region-level adapter. At this stage, the model learns more about region-level knowledge, and as the model is trained on region-level AFTER the image-level, it can avoid severe interference between the two levels.</p> </li> <li> <p>Mixture of the both Finally at the third stage, the model combines the outputs from the previous two stages using a router, which dynamically balances image-level and region-level knowledge. The key component here is the router. The router not only balances between the two levels of knowledge, but also aligns the input-output mismatch by assigning adaptive weights to each adapters’ output based on the task type.</p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/stage-wise_instruction-tuning-480.webp 480w,/assets/img/LION/stage-wise_instruction-tuning-800.webp 800w,/assets/img/LION/stage-wise_instruction-tuning-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/stage-wise_instruction-tuning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 6.</i> The stage-wise instruction-tuning strategy <a href="#1">[1]</a>. </p> \[\begin{align*} O^t = F(X) + \sum_{k=1}^{K=2}G_k^t \odot H_k(X), \end{align*}\] <p>where \(H_k(X)\) is an adapter for \(k\)-th adapter and \(F(X)\) is the output of FFN.</p> <p>To make the stage-wise instruction possible, we must first look at the placement of the adapters in the LLM. Each adapter is inserted at every Feed-Forward Network layer in a parallel manner within the frozen LLM. In this arrangement, the output features generated by the standard FFN are simply added to the output features generated by the adapter layer.</p> <p>Sine each adapter is trained separately, we treat these specialized components as distinct experts.</p> <p>However, as previously mentioned, the router is the key component that enables the model to use these adapters effectively. The router module dynamically decides how much to rely on each adapter based on the task by learning a weight vector G for every task.</p> <p>  For example, if the input involves spatial reasoning, the router increases the contribution of the region-level adapter by updating its weight, and vice versa for global reasoning.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/vision_aggregator-480.webp 480w,/assets/img/LION/vision_aggregator-800.webp 800w,/assets/img/LION/vision_aggregator-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/vision_aggregator.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 7.</i> Vision Aggregator. </p> <p>The next challenge is to ensure that the model can capture sufficient visual details. To capture the fine-grained spatial-aware visual knowledge needed for tasks like visual grounding, the paper introduced a component called the Vision Aggregator.</p> <p>The Vision Aggregator functions as a tiny transformer-style network where that improves LION’s understanding of object boundaries, spatial relations, and fine object attributes.</p> <p>Ablation studies demonstrate that the VA promotes the extraction of this fine-grained knowledge and significantly improves referring expression comprehension (REC) performance.</p> <h3 id="soft-prompting">Soft Prompting</h3> <h4 id="image-tag-extraction">Image Tag Extraction</h4> <p>To improve LION’s capabilities, the paper included semantic comprehension. The authors used an off-the-shelf image tag extractor called Recognize Anything Model (RAM).</p> <h4 id="soft-prompting-1">Soft prompting</h4> <p>Since the predicted tags from this model are not flawless, they can mislead the model. So, LION uses a soft prompt to mitigate the influence of these imperfect tags. A trainable embedding is added to the instruction text that teaches the model how to use the tag information. In the paper, the phrase “According to <i>hint</i>, you are allowed to use or partially use the following tags: …”. This method helps guide the model to select valuable information from the tags and ignore the incorrect ones.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/soft-prompting-480.webp 480w,/assets/img/LION/soft-prompting-800.webp 800w,/assets/img/LION/soft-prompting-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/soft-prompting.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Figure 8.</i> Instruction template with soft prompt <a href="#1">[1]</a>. </p> <h2 id="experimental-results">Experimental Results</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/tab1-480.webp 480w,/assets/img/LION/tab1-800.webp 800w,/assets/img/LION/tab1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/tab1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Table 1.</i> Comparison on image vaptioning and Visual Question Answering (VQA). The best and second performances for each benchmark are indicated in bold and underline, respectively <a href="#1">[1]</a>. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/tab2-480.webp 480w,/assets/img/LION/tab2-800.webp 800w,/assets/img/LION/tab2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/tab2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Table 2.</i> Comparison on Referring Expression Comprehension (REC) <a href="#1">[1]</a>. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LION/tab3-480.webp 480w,/assets/img/LION/tab3-800.webp 800w,/assets/img/LION/tab3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/LION/tab3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <i>Table 3.</i> Evaluation of object hallucination <a href="#1">[1]</a>. </p> <p>The LION architecture effectively addresses the challenge of insufficient visual knowledge extraction and reasoning, which affects existing Multimodal Large Language Models (MLLMs) that typically rely only on coarsely aligned image-text pairs.</p> <p>  The core innovation of LION is the injection of dual-level visual knowledge where, first, the Fine-grained spatial-aware knowledge is incorporated using the mixture-of-adapters using a router and a Vision Aggregator, and, second, the High-level semantic visual evidence is provided by image tags through a soft prompting method.</p> <h2 id="reference">Reference</h2> <p><a id="1" href="https://openaccess.thecvf.com/content/CVPR2024/liquid/Chen_LION_Empowering_Multimodal_Large_Language_Model_with_Dual-Level_Visual_Knowledge_CVPR_2024_paper.html">[1]</a> Chen, G., Shen, L., Shao, R., Deng, X., &amp; Nie, L. (2024). Lion: Empowering multimodal large language model with dual-level visual knowledge. In <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> (pp. 26540-26550).</p> <p><a id="2" href="https://arxiv.org/abs/2306.14824">[1]</a> Peng, Z., Wang, W., Dong, L., Hao, Y ., Huang, S., Ma, S., &amp; Wei, F. (2023). Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824.</p>]]></content><author><name></name></author><category term="paper-review"/><category term="MLLM"/><summary type="html"><![CDATA[Dual-level visual knowledge equipped MLLM with soft prompting]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://3seoksw.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://3seoksw.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>Learn more:Learn more:Learn more:Learn more:Learn more:Learn more:May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><category term="external-posts"/><category term="google"/><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Dynaformer, A Deep Learning Model for Ageing-aware Battery Discharge Prediction</title><link href="https://3seoksw.github.io/blog/2024/Dynaformer-review/" rel="alternate" type="text/html" title="Dynaformer, A Deep Learning Model for Ageing-aware Battery Discharge Prediction"/><published>2024-02-07T00:00:00+00:00</published><updated>2024-02-07T00:00:00+00:00</updated><id>https://3seoksw.github.io/blog/2024/Dynaformer-review</id><content type="html" xml:base="https://3seoksw.github.io/blog/2024/Dynaformer-review/"><![CDATA[<p>The review is done with the following paper:<br/> <a href="https://www.sciencedirect.com/science/article/pii/S0306261923005937">Luca Biggio, Tommaso Bendinelli, Cheta Kulkarni, and Olga Fink. “Dynaformer: A Deep Learning Model for Ageing-aware Battery Discharge Prediction,” <em>Applied Energy</em>, 2023.</a> <d-cite key="dynaformer"></d-cite></p> <h2 id="table-of-contents">Table of Contents</h2> <ul> <li><a href="#abstract">Abstract</a></li> <li><a href="#background">Background</a> <ul> <li><a href="#battery">Battery</a></li> <li><a href="#transformer">Transformer</a></li> </ul> </li> <li><a href="#model-architecture">Model Architecture</a> <ul> <li><a href="#embedding">Embedding</a></li> <li><a href="#encoder">Encoder</a></li> <li><a href="#decoder">Decoder</a></li> </ul> </li> <li><a href="#solutions-for-the-limitations">Solutions for The Limitations</a></li> <li><a href="#results">Results</a> <ul> <li><a href="#experimental-setup">Experimental Setup</a></li> <li><a href="#performance-evaluation">Performance Evaluation</a></li> </ul> </li> <li><a href="#conclusion">Conclusion</a></li> </ul> <h2 id="abstract">Abstract</h2> <p>The main purpose of the paper is to propose a novel deep learning architecture which can be applied to batteries, namely <strong>Dynaformer</strong>. The model, Dynaformer, is a Transformer-based <d-cite key="transformer"></d-cite> model and outputs an EoD (end of discharge) prediction, given some context current and voltage curves.</p> <p>Dynaformer is able to infer the ageing state from a limited samples and predict the full voltage discharge curve, EoD, simultaneously.</p> <h2 id="background">Background</h2> <h3 id="battery">Battery</h3> <p>The following <em>Figure 1.</em> represents how batteries’ voltage curves look like based on the given input current profiles. When given constant current, it forms a continuous curve as the graph on the far left. On the other hand, when given some variable current profiles with multiple transitions, the graphs differ from the other corresponding to the altering current.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig1-480.webp 480w,/assets/img/dynaformer/fig1-800.webp 800w,/assets/img/dynaformer/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <i>Figure 1.</i> Varying voltage curves corresponding to altering current profiles <d-cite key="dynaformer"></d-cite> </div> <p>There are two main categories of solving EoD prediction and ageing inference: model-based method and data-driven method. The first method, model-based method, mainly focuses on representation of battery’s physical internal mechanism. By constructing an ideal model of the battery, it has a good performance. However, due to it’s complex representation of the model, it is computationally expensive and is not an easy work to design the model precisely.<br/> The second method, data-driven method, is to predict EoD and ageing inference using a huge amount of battery data. It is easier to model the system compared to the model-based method since it requires minimum prior knowledge on the battery’s discharge and degradation processes. Nevertheless, it also has some disadvantages. It requires large labeled training dataset and it is inefficient to train with such long time series data.</p> <h3 id="transformer">Transformer</h3> <p>It is essential to understand how <strong>Transformer</strong> <d-cite key="transformer"></d-cite> works in order to understand the Dynaformer. Let’s consider the following sentence.</p> <p align="center"> <b>The dog</b> is playing and <b>she</b> likes playing with <b>me</b>. </p> <p>We know that <b>she</b> is indicating <b>the dog</b>, not <b>me</b>. Transformer is all about understanding the context and the hidden meaning behind the given data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig2-1-480.webp 480w,/assets/img/dynaformer/fig2-1-800.webp 800w,/assets/img/dynaformer/fig2-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig2-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig2-2-480.webp 480w,/assets/img/dynaformer/fig2-2-800.webp 800w,/assets/img/dynaformer/fig2-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig2-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig2-3-480.webp 480w,/assets/img/dynaformer/fig2-3-800.webp 800w,/assets/img/dynaformer/fig2-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig2-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <i>Figure 2.</i> Transformer model architecture <d-cite key="transformer"></d-cite> (Left) Positional embedding (Centre) Encoder (Right) Decoder </div> <p>Transformer is consists of three major parts: positional embedding, encoder and decoder. The above <em>Figure 2.</em> represents the mentioned three parts.<br/> Positional embedding is to transfer the given data into numerical vectors. By doing so, positional information can be embedded within the vectors.<br/></p> <p>Encoder’s job is to obtain <strong>query</strong>, <strong>key</strong> and <strong>value</strong> – \((Q, K, V)\) – given some positional embedding. The <strong>query</strong> is a vector which contains given specific data such as a word itself when a sentence is given. The <strong>key</strong> is a value which can specify the <strong>query</strong>. And lastly, the <strong>value</strong> represents the <strong>query</strong>’s hidden meaning. It can contain context or positional information. Here, <strong>self-attention</strong> comes in. Simply saying, self-attention cells find the correlations among the data by using \((Q, K, V)\).<br/></p> <p>Decoder’s job is very similiar with the encoder but it differs with the main purpose. While the encoder’s main purpose is to find the correlations among input data, the decoder’s main purpose is to find the correlations between the input data and ouput data.<br/> To simplify, let’s bring the classic translation example. Say I want to translate some English sentences into Korean sentences. Then the input of the encoder is the English sentences and the input of the decoder is the the Korean sentences, in other words, data that we target. So the encoder mainly interprets and finds the meaning, hidden information, and correlations from the English sentences, and the decoder focuses on finding the correlations between the English sentences and the Korean sentences, when training.<br/> Back to the point, the decoder also obtains <strong>query</strong>, <strong>key</strong>, and <strong>value</strong>. However, in the decoder, <strong>key</strong> and <strong>value</strong> from the encoder and <strong>query</strong> from decoder will only be used. By using \((K, V)\) from the encoder and \((Q)\) from the decoder, the decoder is able to apply self-attention mechanism to find the correlations between the data we want to interpret and the data we aims.</p> <h2 id="model-architecture">Model Architecture</h2> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig2-3-480.webp 480w,/assets/img/dynaformer/fig2-3-800.webp 800w,/assets/img/dynaformer/fig2-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig2-3.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 3.</i> Representation of the components of Dynaformer <d-cite key="dynaformer"></d-cite> </div> <p>The proposed Dynaformer is basically the same with the Transformer model but with the difference of the data type. Since the paper is to predict EoD, input data types are current and voltage curves. Note that the inputs of the encoder are current and voltage curves and the inputs of the decoder are the rest of the current curves and the output of the encoder, then eventually outputs full discharge voltage curves.</p> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig4-480.webp 480w,/assets/img/dynaformer/fig4-800.webp 800w,/assets/img/dynaformer/fig4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig4.jpg" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 4.</i> Dynaformer - model architecture <d-cite key="dynaformer"></d-cite> &lt;/caption&gt; _Figure 3._ and _Figure 4._ are the same model architecture, but for the sake of easy understanding of the Dynaformer using the Transformer-style architecture representation, _Figure 3._ can be redrawn as _Figure 4._. The following figures specify each part from the Dynaformer. <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig4-1-480.webp 480w,/assets/img/dynaformer/fig4-1-800.webp 800w,/assets/img/dynaformer/fig4-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig4-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig4-2-480.webp 480w,/assets/img/dynaformer/fig4-2-800.webp 800w,/assets/img/dynaformer/fig4-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig4-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig4-3-480.webp 480w,/assets/img/dynaformer/fig4-3-800.webp 800w,/assets/img/dynaformer/fig4-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig4-3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p align="center"> <i>Figure 4-1, 4-2, 4-3.</i> Dynaformer - detailed model architecture <d-cite key="dynaformer"></d-cite> (Left) Positional embedding (Centre) Encoder (Right) Decoder </p> ### Embedding <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig5-480.webp 480w,/assets/img/dynaformer/fig5-800.webp 800w,/assets/img/dynaformer/fig5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig5.jpg" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 5.</i> Dynaformer - (Context) Embedding <d-cite key="dynaformer"></d-cite> </div> Here, Dynaformer gets information regarding the battery's profiles, which are current profile (curve) and voltage profile (curve). While (positional) embedding, nothing special happens but reshapes the data into $$(Q, K, V)$$ including positional, context information. The $$Q$$ represents current and voltage curves, the $$K$$ serves as a specifier to find the $$Q$$, and the $$V$$ contains positional information such as time for the matching $$Q$$. ### Encoder <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig6-480.webp 480w,/assets/img/dynaformer/fig6-800.webp 800w,/assets/img/dynaformer/fig6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig6.jpg" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 6.</i> Dynaformer - Encoder </div> The encoder's inputs are $$(Q, K, V)$$ from the embedding. Here, it's main role is to find the correlations between the input current, voltage, and time which will eventually extracts degradation information. In order to find such information, multi-head self-attention cells are used. For further information regarding self-attention mechanism, please see <d-cite key="transformer"></d-cite> ### Decoder <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig7-480.webp 480w,/assets/img/dynaformer/fig7-800.webp 800w,/assets/img/dynaformer/fig7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig7.jpg" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 7.</i> Dynaformer - Decoder <d-cite key="dynaformer"></d-cite> </div> The decoder predicts EoD as a final ouput, given the ageing inference from the encoder's ouput and the rest of the current curves which are from the decoder's input. Using the ageing inference, $$(K, V)$$, and the rest of the current curves, $$Q$$, the Dynaformer is now able to predict the voltage curves corresponding to the current curves from the input of the decoder exploiting the ageing inference information from the encoder. ## Solutions for The Limitations Just like I've mentioned from the [Background - Battery](#battery) section, there are two main limitations when using data-driven method for predicting EoD and ageing inference: requires large labeled dataset and requires long time series dataset which is inefficient for training.<br/> The proposed Dynaformer solves the problems originally had when using data-driven method. First, the Dynaformer mitigated the problem of lack of dataset for training by using the following: <d-cite key="teubert2023progpy"></d-cite> and <d-cite key="saha2007battery"></d-cite>. <d-cite key="teubert2023progpy"></d-cite> is a NASA Prognostics Model library which helps creating a simulated training dataset. The paper experiments the model by changing two degradation parameters, $$q_{max}$$ and $$R_0$$, to observe the performance change when the parameters changed. Also <d-cite key="saha2007battery"></d-cite> is a NASA real-world Dataset for batteries and is used as a input for the simulator, <d-cite key="teubert2023progpy"></d-cite>.<br/> However, there may be a simulation-to-real gap (sim2real gap) since the training dataset is a simulation-based data. To mitigate such concern, the paper applied transfer learning. As <d-cite key="sun2019fine"></d-cite> suggests, training the model with simulated data then using a limited amount of real data to adapt the model can reduce the model's bias towards simulated data. Second, the Dynaformer gets the **tokenized** input. In other words, instead of feeding a full length of the curves, the Dynaformer only accepts a small sized curves in sequence. Please see _Figure 3._ for better understanding of the concept of **token**. Training the model with long time series data is in fact inefficient. However, using the technique from <d-cite key="dosovitskiy2020image"></d-cite> solves such problem. ## Results ### Experimental Setup Two metrics will be used to evaluate the performance of the model namely _RMSE_ (Root Mean Squared Error) and _RTE_ (Relative Temporal Error). You might be familiar with RMSE but not RTE. The RTE is a error evaluating metric, which the paper proposed, inspecting the maximum error when given a longer or shorter input profile. It measures how the model behaves when the input size is longer or shorter than usual measuring the maximum error, so the authors call the RTE metric a worst case performance measurement.<br/> The following is the algorithm for measuring the RTE metric <d-cite key="dynaformer"></d-cite>. <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/algo1-480.webp 480w,/assets/img/dynaformer/algo1-800.webp 800w,/assets/img/dynaformer/algo1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/algo1.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> ### Performance Evaluation Please see the following _Table 1._ for the evaluation. As described from [Results - Experimental Setup](#experimental-setup) section, two metrics, RMSE and RTE, will be used to evaluate the performance of the model. For the comparison, LSTM model and FNN model will be used for simulated data. Note that since FNN model can not handle variable-sized input, only the LSTM model will be compared. And for the final evaluation, real data will be used to see the performance of the Dynaformer model using two metrics. <table> <tr> <td rowspan="3"></td> <td rowspan="3"></td> <td rowspan="3">LSTM</td> <td rowspan="3">FNN</td> </tr> <tr> <td colspan="2">Metrics</td> </tr> <tr> <td>RMSE</td> <td>RTE</td> </tr> <tr> <td rowspan="3">Simulated data</td> </tr> <tr> <td>Constant current</td> <td>O</td> <td>O</td> <td>X</td> <td>O</td> </tr> <tr> <td>Variable current</td> <td>O</td> <td>X</td> <td>O</td> <td>O</td> </tr> <tr> <td colspan="2">Real data</td> <td>X</td> <td>X</td> <td>O</td> <td>O</td> </tr> </table> <div class="caption"> <i>Table 1.</i> Performance evaluation </div> #### Simluated Constant Current Profiles <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig8-480.webp 480w,/assets/img/dynaformer/fig8-800.webp 800w,/assets/img/dynaformer/fig8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig8.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 8.</i> <b>Results on constant load profiles</b> <d-cite key="dynaformer"></d-cite> (Left) Interpolation performance. Altering degradation values in the same range to the training set. (Right) Extrapolation performance. Altering degradation values in the different range to the training set. Dynaformer with a asterisk (*) denotes the model trained with variable profiles. </div> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig9-480.webp 480w,/assets/img/dynaformer/fig9-800.webp 800w,/assets/img/dynaformer/fig9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig9.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 9.</i> <b>Generalization performance analysis</b> with respect to degradation parameters. <d-cite key="dynaformer"></d-cite> (Left) Interpolation performance. Altering degradation values in the same range to the training set. (Right) Extrapolation performance. Altering degradation values in the different range to the training set. Dynaformer with a asterisk (*) denotes the model trained with variable profiles. &lt;/p&gt; <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig9-480.webp 480w,/assets/img/dynaformer/fig9-800.webp 800w,/assets/img/dynaformer/fig9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig9.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 9.</i> <b>Generalization performance analysis</b> with respect to degradation parameters. <d-cite key="dynaformer"></d-cite> The grey-coloured area represents the interpolation region. Fixed current of 1A (left) and 2A (right). </div> #### Simluated Variable Current Profiles <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig10-480.webp 480w,/assets/img/dynaformer/fig10-800.webp 800w,/assets/img/dynaformer/fig10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig10.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 10.</i> <b>Results on variable load profiles</b> using Dynaformer <d-cite key="dynaformer"></d-cite> (Left) Interpolation performance. Altering degradation values in the same range to the training set. (Right) Extrapolation performance. Altering degradation values in the different range to the training set. </div> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig11-480.webp 480w,/assets/img/dynaformer/fig11-800.webp 800w,/assets/img/dynaformer/fig11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig11.jpg" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 11.</i> <b>Illustration of the Dynaformer's prediction</b> with respect to the numbers of transitions of current <d-cite key="dynaformer"></d-cite> </div> #### Implicit Ageing Inference <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig12-480.webp 480w,/assets/img/dynaformer/fig12-800.webp 800w,/assets/img/dynaformer/fig12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig12.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 12.</i> <b>Implicit parameters inference</b> <d-cite key="dynaformer"></d-cite> With the use of encoder's output information, two principal components, one can infer the corresponding degradation parameters by inspecting the following areas. (Left) Shows high correlations between q_max and the second principal component. (Right) Shows high correlations between R_0 and the first principal component. </div> #### Real data <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dynaformer/fig13-480.webp 480w,/assets/img/dynaformer/fig13-800.webp 800w,/assets/img/dynaformer/fig13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dynaformer/fig13.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <i>Figure 13.</i> <b>Adaption to real data via fine-tuning</b> <d-cite key="dynaformer"></d-cite> Close the gap between simulation-to-real gap by showing small fraction of real data to the pre-trained model. (a) Sim2Real gap (b) Performance before / after fine-tuning (c) MSE distribution before / after fine-tuning (d) RTE measurements according to the number of real data used to train </div> **Note**. Optimal performance can already be obtained with smaller training sizes. ## Conclusion - Dynaformer (Transformer-based EoD prediction model) - Trained with large simulated data using <d-cite key="teubert2023progpy"></d-cite> and <d-cite key="saha2007battery"></d-cite>. - Fine-tuned with small amount of real data in order to close sim2real gap - able to interpret ageing inference easily with the output of the encoder - Ultimately, provides very precise prediction of EoD - Proposed three possible extension works: 1. Apply the proposed methodology to alternative simulators 2. Exploit the characteristic that the model represents _differentialbe_ simulator - Gradient-based methods can be used to specify when the voltage trajectory leads to discharge 3. Apply the Dynaformer to learning very different system dynamics </div></div>]]></content><author><name></name></author><category term="Transformer"/><category term="Transformer"/><summary type="html"><![CDATA[Transformer-based Battery EoD Prediction Model]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://3seoksw.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry></feed>