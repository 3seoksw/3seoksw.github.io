<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Double DQN implementaion on Super Mario Bros. | WooSeok Kim </title> <meta name="author" content="WooSeok Kim"> <meta name="description" content="DDQN implementaion using Mario (gym) API"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://3seoksw.github.io/projects/DDQN-Mario/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">WooSeok</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Double DQN implementaion on Super Mario Bros.</h1> <p class="post-description">DDQN implementaion using Mario (gym) API</p> </header> <article> <h1 id="double-dqn-super-mario-bros">Double DQN: Super Mario Bros.</h1> <p><a href="https://github.com/3seoksw/DDQN-mario" rel="external nofollow noopener" target="_blank">This project</a> is built based on the paper regarding <b>Double DQN</b><a href="#1">[1]</a> and official <strong>PyTorch</strong> website <a href="#2">[2]</a>.<br> The main motivation and purpose of building this project was to enhance the better understanding of how reinforcement learning works on practice through code. Here, the code applies DDQN which is quite similar to DQN <a href="#3">[3]</a>. For further information regarding the difference between the two, please see the following: <a href="#background">Background</a> or <a href="#1">[1]</a>.</p> <h2 id="showcase">Showcase</h2> <p align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mario/mario_1_1_showcase-480.webp 480w,/assets/img/mario/mario_1_1_showcase-800.webp 800w,/assets/img/mario/mario_1_1_showcase-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mario/mario_1_1_showcase.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </p> <h2 id="to-get-started">To Get Started</h2> <div class="language-zsh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:3seoksw/DDQN-mario.git
</code></pre></div></div> <p>In order to properly run the code, first clone the repository.<br> And create new virtual environment using the <code class="language-plaintext highlighter-rouge">requirements.txt</code>. Here, I’m using <code class="language-plaintext highlighter-rouge">conda</code>.<br></p> <div class="language-zsh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> &lt;your-env&gt; <span class="nt">--file</span> requirements.txt
conda activate &lt;your-env&gt;
</code></pre></div></div> <p>After setting up the virtual environment and the code, run <code class="language-plaintext highlighter-rouge">main.py</code>.<br></p> <p><strong>Note</strong>: <a href="https://pypi.org/project/nes-py/" rel="external nofollow noopener" target="_blank">nes-py 8.2.1</a> (from <a href="https://pypi.org/project/gym-super-mario-bros/" rel="external nofollow noopener" target="_blank">gym-super-mario-bros 7.4.0</a>) and <a href="https://gymnasium.farama.org/content/migration-guide/" rel="external nofollow noopener" target="_blank">gym 0.26.0</a> are not compatible. There is a difference in <code class="language-plaintext highlighter-rouge">reset()</code> function’s signature between two APIs.<br> There are few workarounds but I erased every <code class="language-plaintext highlighter-rouge">truncated</code> keyword in <code class="language-plaintext highlighter-rouge">time_limit.py</code> from imported package directory.</p> <h2 id="problem-justification">Problem Justification</h2> <p>Because the project is using several packages, explicit (manual) definition or assignment of \(\text{Environment, State, Action, Reward}\) is not necessary.<br> But for the sake of understanding of RL, you might want to see <a href="#3">[3]</a> which provides detailed explanations on solving Atari games using RL.<br></p> <p>The following explains how problem can be justified in a simple manner:</p> \[\begin{align*} \text{Environment}: &amp;\text{the world agent interacts with} \\ &amp; \textit{i.e.} \text{) stage, blocks, mushrooms, } \textit{etc.} \\ \text{State}: &amp;\text{current image frame} (\text{channel} \times \text{height} \times \text{width}) \\ \text{Action}: &amp;\text{set of actions agent Mario can take} \\ &amp;\textit{e.g.}) \text{move forward, jump, } \textit{etc.} \\ \text{Reward}: &amp;\text{distance agent moved} \\ &amp;\text{coins agent acquired} \\ &amp;\text{enemies agent killed} \\ &amp;\text{time consumed} \\ &amp;\text{reached to the final flag (terminal state)} \\ &amp;\textit{etc.} \\ \end{align*}\] <p>Main goal is to agent maximizing its rewards.</p> <p>Since states are in the forms of image frames, CNN will be used which consists of three convolutional layers paired with ReLU function, flattening layer, densifying the tensor processed, and two fully-connected neural networks.<br> Agent will judge the current situation using the state information</p> <p>Actions can vary (right-only, simple, or complex) according to <a href="https://pypi.org/project/nes-py/" rel="external nofollow noopener" target="_blank">nes-py 8.2.1</a>. Agent will choose an action based on the algorithm namely DDQN in order to maximize the reward.</p> <p>Rewards are the key to solve RL problems. Agent will take actions based on the rewards. Here, rewards can be whether the agent reached to final state (flag), the distance agent moved, etc.</p> <h2 id="background">Background</h2> <p>There are some similarities between DQN and Double DQN (DDQN) in terms of both taking advantage of using \(Q\) values. But there is a major difference for the method to update \(Q\) value. For further reference, please see <a href="#1">[1]</a>.</p> <h3 id="dqn">DQN</h3> <p>In \(Q\)-learning, objective is to find optimal \(Q\) value which is parameterized by \(\theta\). The \(Q\)-learning update requires some action \(A_t\), state \(S_t\), and reward \(R_{t+1}\), then we can get:</p> \[\begin{aligned} \theta_{t+1} &amp;= \theta_t + \alpha(Y^{Q}_t - Q(S_t, A_t; \theta_t)) \nabla_{\theta_t}Q(S_t, A_t; \theta_t) \end{aligned}\] <p>where \(\alpha\) is a step size. And the target \(Y_t^Q\) is defined as:</p> \[\begin{aligned} Y_t^Q \equiv R_{t+1} + \gamma \text{max}_aQ(S_{t+1}, a; \theta_t) \\ Y^Q_t \approx Q(S_t, A_t; \theta_t) \end{aligned}\] <p>However, the target value can cause overestimation. Therefore, DDQN is proposed.</p> <h3 id="double-dqn">Double DQN</h3> <p>While applying the same fundamental foundations from DQN, experience replay and target network, DDQN uses two separate \(Q\)-networks; online network which is for selecting the best action and target network which is for evaluating the action. You can simply think DDQN is separating DQN’s target network into two.</p> <p>The target is as follows:</p> \[\begin{aligned} Y_t^{\text{DQN}} \equiv R_{t+1} + \gamma \text{max}_a Q(S_{t+1}, a; \theta_t^{-}) \end{aligned}\] <p>where \(\theta^{-}\) is a vector parameters of target network. And the Double \(Q\)-learning error can be written as follows:</p> \[\begin{aligned} Y_t^{\text{DoubleQ}} \equiv R_{t+1} + \gamma Q_{\text{eval}}^{\text{target}}(S_{t+1}, \text{argmax}_a Q_{\text{select}}^{\text{online}}(S_{t+1}, a; \theta_t); \theta_t^{-} ) \end{aligned}\] <p>where \(\theta\) is parameterizing online network.</p> <h2 id="references">References</h2> <p><a id="1" href="https://ojs.aaai.org/index.php/AAAI/article/view/10295" rel="external nofollow noopener" target="_blank">[1]</a> H. v. Hasselt, A. Guez, and D. Silver. “Deep Reinforcement Learning with Double Q-learning,” <i>Proceedings of the AAAI Conference on Artificial Intelligence, 30(1)</i>, 2016.</p> <p><a id="2" href="https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.liquid" rel="external nofollow noopener" target="_blank">[2]</a> Y. Feng, S. Subramanian, H. Wang, and S. Guo. “TRAIN A MARIO-PLAYING RL AGENT,” PyTorch, accessed January 27, 2024, https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.liquid.</p> <p><a id="3" href="https://arxiv.org/abs/1312.5602" rel="external nofollow noopener" target="_blank">[3]</a> V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. “Playing atari with deep reinforcement learning.” <i>arXiv preprint arXiv:1312.5602</i>, (2013).</p> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 WooSeok Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>